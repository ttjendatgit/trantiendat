[{"uri":"https://ttjendatgit.github.io/trantiendat/en/4-eventparticipated/4.1-event1/","title":"AWS CLOUD DAY","tags":[],"description":"","content":"Summary Report: “AWS Cloud, AI \u0026amp; Innovation Summit” Event Objectives Showcase Vietnam’s national strategies for cloud growth and digital transformation Deepen the U.S.–Vietnam collaboration in technology and innovation Share perspectives on AI, blockchain, and ecosystem development influencing Vietnam’s future Highlight AWS programs in talent development, cloud accessibility, and responsible AI initiatives Deliver hands-on technical knowledge in AI-powered development and AI security Speakers Representative of the Vietnam Government U.S. Ambassador to Vietnam Eric Elock – CEO for Vietnam, Laos, Cambodia \u0026amp; Myanmar Chloe Phung – CEO, U2U Erik – AWS Leadership Jaime Valless – AWS Leadership AWS Technical Specialists – Leading afternoon deep-dive sessions Key Highlights National strategy on cloud infrastructure and digital transformation The government emphasized expanding cloud services and digital systems as the backbone of Industry 4.0 Ensures security, safety, and information protection across national digital ecosystems Encourages open collaboration between public institutions, private companies, and global investors Positions cloud technology as a catalyst for economic development and modernization U.S.–Vietnam relationship and technological development The U.S. Ambassador reflected on three decades of partnership between the two countries Technology firms such as AWS act as key partners driving co-development Focus placed on shared economic growth and sustained long-term cooperation Innovation through banking modernization \u0026amp; blockchain ecosystems – Eric Elock The banking sector continues to be essential in driving IT modernization U2U is building a blockchain-powered ecosystem enabling seamless business and user interaction Highlights how the combination of cloud and blockchain is defining new digital economic models AI shaping Vietnam’s future – Chloe Phung Concepts once labeled “impossible” for U2U two years ago have now materialized Vietnam is not only keeping pace with global AI trends but actively shaping the AI era Real impacts of AI in Vietnam Education:\n60% of Vietnamese students now adopt EdTech learning tools AI improves accessibility, reduces language obstacles, and increases engagement Economy:\nMore than 765 AI startups, making Vietnam 2nd in ASEAN AI projected to contribute $120–130 billion to GDP Social impact:\nHospitals using AI have reduced patient processing time to 5 minutes AI enhances traffic optimization, energy monitoring, and coastal protection systems Technology examples Nubila – AI-based weather forecasting Staex – Successfully deployed 1,000+ IoT devices across Asia and Europe AI \u0026amp; blockchain synergy Generative AI shortens development cycles from weeks to hours or days Blockchain becomes easier to adopt, even for beginners, when paired with AI AI improves decision-making for both businesses and policymakers Acknowledgment given to AWS for enabling this technological ecosystem AWS initiatives for Vietnam – Erik AWS has trained over 100,000 cloud practitioners in Vietnam Continues to expand cloud service accessibility nationwide Launched the FJC 6-month program, providing structured pathways to tech careers Emphasized AWS’s strong cultural values as a core advantage Where culture meets innovation – Jaime Valless Humanity is entering a pivotal era where AI will reshape every industry AI transformation requires not just technology, but also skills, people, culture, and responsibility Encouraged ongoing learning and ethical use of AI AWS supports secure AI deployment with multi-model access and protection layers Use case example Nearmap: Uses AI models to accelerate decision-making by automating repetitive tasks, freeing humans to focus on creativity and strategic thinking Key Takeaways Design Mindset Cloud, AI, and blockchain together unlock major new innovation opportunities Collaboration between government, enterprises, and global partners drives rapid digital advancement AI deployment must prioritize responsibility, security, and human oversight Technical Architecture AI accelerates software development through automated coding, testing, and optimization IoT and weather-modeling cases illustrate AI’s cross-industry scalability Generative AI lowers barriers to understanding complex technologies like blockchain Modernization Strategy Apply cloud + AI + blockchain for impactful transformation Invest in ongoing digital skills training Maintain responsible AI practices: access control, secure prompting, hallucination mitigation, data safeguards Applying to Work Adopt AI-centered development workflows using AWS tools Integrate responsible AI guidelines: user tracking, human-in-the-loop, prompt security, data validation Consider RAG architectures for accurate and secure enterprise AI Experiment with services like Amazon Q and QuickSight for rapid prototyping and visual dashboards Event Experience Attending the “AWS Cloud, AI \u0026amp; Innovation Summit” offered comprehensive insights into how cloud, AI, and blockchain technologies are accelerating Vietnam’s digital transformation.\nLearning from industry leaders Government officials, international representatives, and AWS leaders shared strategic visions Concrete examples demonstrated AI’s impact across education, economic growth, and public services Hands-on technical exposure Afternoon sessions covered AWS SageMaker, AI-driven SDLC, and AI application security\nShowcased a complete AI-enhanced development workflow:\nInception: idea generation, requirement gathering Construction: modeling, code generation, testing, IaC deployment Operation: production rollout and incident response Security practices for AI applications Discussed risks including hallucination, data poisoning, prompt vulnerabilities, and access control Provided secure architectural patterns for AI deployment, supply chain protection, RAG usage, and user monitoring Leveraging AWS tools Amazon Q and QuickSight streamline dashboard building and workflow automation AI assists with coding, documentation, refactoring, and more—boosting productivity while maintaining human supervision Lessons learned AI and cloud technologies drive efficiency and innovation across diverse sectors Robust security, responsible practices, and human oversight are indispensable Vietnam is experiencing strong momentum in AI adoption and digital transformation Some event photos Figure 1 Figure 2 Figure 3 In summary, the summit delivered strategic perspectives, technical depth, and real-world use cases illustrating how AI, cloud computing, and blockchain are shaping Vietnam’s digital future.\n"},{"uri":"https://ttjendatgit.github.io/trantiendat/en/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members in the First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Complete Module 1 theory and related Labs. Tasks to be implemented this week: Day Tasks Start Date End Date Reference Materials 2 - Interact with FCJ members in the group chat.\n- Find members for the team, create a group chat to receive notifications and interact with members.\n- Read and note the rules and regulations at the internship unit. 09/09/2025 09/09/2025 3 - Learn about AWS, types of services and their practical applications in real projects and work environments. - Watch tutorials on drawing AWS architecture on draw.io and download AWS icons. - Watch Workshop tutorials and download related resources: + Hugo + Snagit + ActivePresenter + Git + \u0026hellip; - Follow the Workshop instructions. 10/09/2025 10/09/2025 https://www.youtube.com/watch?v=mXRqgMr_97U\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=3 https://www.youtube.com/watch?v=l8isyDe-GwY\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=2 4 - Complete Module 01 theory and practice Labs: + Module 01-Lab01-01 to Lab01-04 + Module 01-Lab07-01 to Lab07-04 - Practice: + Create AWS account, verify MFA code. + Create Admin Group and Admin User. + Support account verification. + Create Budget by Template, Usage Budget, Cost Budget and Reservation Instance (RI) Budget. 11/09/2025 11/09/2025 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - Methods to SSH into EC2 - Learn about Elastic IP - Meet with the group to discuss the project - Practice: + Module01-Lab07-05 to Lab07-06 + Module01-Lab09-01 to Lab09-03 12/09/2025 12/09/2025 Week 1 Achievements: Connected and interacted with members in First Cloud Journey, created a group chat for exchange, receiving notifications, and effective work coordination.\nRead, memorized, and complied with the rules and regulations at the internship unit.\nUnderstand AWS overview, main service groups (Compute, Storage, Networking, Database, …) and their application in real projects.\nCompleted Module 01 theory and all related labs (Lab01-01 to Lab01-04, Lab07-01 to Lab07-06, Lab09-01 to Lab09-03).\nSuccessfully performed practical operations:\nCreated and verified AWS Free Tier Account, enabled MFA security.\nCreated Admin Group, Admin User, assigned administrative permissions.\nSet up Budgets (Usage Budget, Cost Budget, RI Budget).\nFamiliarized with AWS Management Console and AWS CLI, know how to install, configure Access Key, Secret Key, Region.\nUsed AWS CLI to check account information, list regions, view EC2 services, create and manage key pairs.\nLearned details about EC2: Instance Types, AMI, EBS, Elastic IP, SSH connection methods.\nHad team meetings to discuss internship project direction and task allocation.\n"},{"uri":"https://ttjendatgit.github.io/trantiendat/en/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Enhancing Telecommunications Security with AWS Authors: Kal Krishnan, Danny Cortegaca and Ruben Merz on 07 FEB 2025 in Best Practices, Intermediate (200), Security, Identity, \u0026amp; Compliance, Thought Leadership Permalink\nImplementing CISA\u0026rsquo;s Guidance for Enhanced Visibility and Hardening of Telecommunications Infrastructure In response to recent cybersecurity incidents by actors from the People\u0026rsquo;s Republic of China, multiple cybersecurity agencies led by the U.S. Cybersecurity and Infrastructure Security Agency (CISA) have jointly issued comprehensive guidance for securing telecommunications infrastructure. As communications service providers (CSPs) move their workloads to the cloud, they must take steps to effectively implement these security measures in the cloud environment.\nThis blog post describes how CSPs can use Amazon Web Services (AWS) features to implement this guidance while benefiting from the cloud\u0026rsquo;s advantages.\nThe guidance focuses on two main areas:\nEnhanced Visibility: Enabling security teams to monitor, detect, and respond to potential threats through comprehensive insight into digital assets.\nEnhanced Hardening of Systems and Devices: Implementing robust security controls and configurations to mitigate vulnerabilities and help prevent unauthorized access.\nOverview of Cloud Fundamentals Before delving into the specific guidance in this post, it\u0026rsquo;s important to understand how security recommendations apply differently to public cloud environments versus private infrastructure. A common trend in the telecommunications industry is to view the public cloud as merely an extension of private clouds. This can lead to misunderstandings about security capabilities and inefficient use of native public cloud security features.\nThe fundamental difference lies in how public cloud architecture—especially designed for multi-tenancy—is built on a foundation of strong tenant isolation as a core design principle.\nIn AWS, virtual resources are isolated by default and require explicit configuration to connect. For example, when you create a virtual private cloud (VPC) using Amazon VPC, this logically isolated network does not permit traffic in or out until specific routes and gateways are intentionally configured. Similarly, Amazon Simple Storage Service (Amazon S3) buckets are private by default, requiring explicit configuration to grant access. This isolation extends to our core virtualization infrastructure through the AWS Nitro System, which provides unprecedented workload isolation—even AWS\u0026rsquo;s most privileged operators have no technical access to customer workloads. Furthermore, data moving between virtual machines running on the Nitro System or across our global backbone is automatically encrypted, providing additional layers of protection beyond customer-deployed encryption.\nThis security-by-design and security-by-default philosophy permeates AWS service design and operations. It\u0026rsquo;s not merely a design choice—it\u0026rsquo;s a business imperative driven by the operational resilience imperative and customer trust in the public cloud model. Our commitment to these principles is demonstrated by our signing of CISA\u0026rsquo;s Secure by Design pledge.\nWhen AWS customers operate on the public cloud platform, understanding the shared responsibility model is crucial. This model clearly delineates security responsibilities: AWS is responsible for security of the cloud, while the customer is responsible for security in the cloud. This division of responsibility significantly reduces your operational burden because AWS assumes responsibility for securing everything pertaining to and within the cloud services they provide, from physical protection of data centers onward. This allows you to focus your security resources on what matters most—protecting your applications and workloads—while AWS handles the undifferentiated heavy lifting of infrastructure security.\nThis shared responsibility model becomes even more beneficial when considering the inherent economies of scale in public cloud operations. AWS\u0026rsquo;s massive scale enables us to invest more in securing the platform than any single enterprise could achieve independently, creating a security multiplier effect that benefits all customers. A compelling example of this scale advantage is our comprehensive threat intelligence program, which deploys honeypot sensors across our entire global network. These sensors observe over 100 million potential threat interactions and probes daily. Using artificial intelligence and machine learning (AI/ML), we analyze this information and take rapid, often automated actions to mitigate threats. In the first half of 2023 alone, this program allowed us to analyze the origin of roughly 230,000 Layer 7 distributed denial-of-service (DDoS) events. We also provide this intelligence to customers through services like Amazon GuardDuty, extending the benefits of our scale to customers.\nAWS\u0026rsquo;s operational scale not only enables exemplary threat intelligence gathering but also necessitates comprehensive automation of security operations. Routine tasks, such as feature and patch deployment and configuration updates, are completely automated through deployment processes. Automation provides the added benefit of removing humans from the loop, thereby minimizing the risk of error.\nOur scale also facilitates comprehensive compliance with security standards across multiple industries and jurisdictions. Our global presence and diverse customer base require adherence to the strictest security requirements worldwide. Through the AWS Compliance Program, we have achieved 143 security standards and compliance certifications, from ISO standards for security and privacy on a cloud platform to industry-specific regulations in finance and healthcare, and government security programs. This includes independent verification of our claims about the isolation properties of the Nitro System virtualization infrastructure. As a result, you benefit from compliance at scale, having access to secure, certified infrastructure deployed with advanced security systems.\nThis is why, in the blog post titled Why Cloud-First Is Not a Security Concern, the UK\u0026rsquo;s National Cyber Security Centre concluded that \u0026ldquo;using the cloud securely should be your main concern—not the underlying security of the public cloud.\u0026rdquo;\nOn the other hand, private clouds are typically under the control of a single organization and are single-tenant, providing relatively weak workload isolation. Virtual resources in private clouds are often connected by default upon creation, requiring specific steps to enhance isolation. Manual operations, with their potential for error and possible threat actor involvement, often still dominate private cloud workflows. They rarely undergo the level of security scrutiny that public clouds regularly face. These, along with other differences, mean that the security risk profile of each service is inherently different, necessitating separate security controls and solution architectures to mitigate these risks.\nImplementing the Security Hardening Guidance with AWS Your cloud resources and data reside within an AWS account. The account serves as an isolation boundary for identity and access management. When you need to share resources and data between two accounts, you must explicitly grant this access. This helps mitigate the risk of lateral movement between accounts.\nProperly designing your AWS environment establishes a strong foundation for meeting CISA\u0026rsquo;s cybersecurity guidance. AWS Control Tower, working with AWS Organizations, allows you to set up a well-architected multi-account environment based on security best practices.\nFor detailed guidance on creating a secure landing zone for telecommunications workloads, refer to our comprehensive blog post on the topic.\nWe have analyzed the recommendations in CISA\u0026rsquo;s guidance and grouped them into six categories under the two main focus areas. Please refer to the linked GitHub page at the end of this article for more detailed guidance on relevant AWS services that can help you implement each individual security measure in the guidance.\nLogging and Monitoring Guidance in this category emphasizes the importance of enhanced visibility to understand network activity, which is essential for anomaly detection and incident response. Key security controls include: having robust asset management capability, enabling logging at multiple levels, centralizing logging, protecting logging and monitoring infrastructure, and using security tools to detect anomalies and incidents.\nEnhanced visibility is an inherent advantage of the public cloud model, particularly in AWS. This transparency is not merely an added feature but a fundamental need driven by the API-centric, pay-for-use business model. To bill customers accurately, AWS has integrated comprehensive tracking and logging functions into its core architecture. Consequently, AWS provides robust tools that enable you to collect, centralize, and monitor logs across every layer of your network workload. This level of visibility far exceeds what is typically achieved in traditional infrastructure, giving you unprecedented insight into IT assets and user activity.\nAnother important guidance is to centralize security-related logging and separate logging infrastructure from other production environments. You can implement this guidance in AWS using Amazon Security Lake alongside OpenSearch deployed in separate accounts, with access restricted to the security organization only. Alternatively, this solution provides a best-practice deployment method for creating collection and ingestion pipelines to enable centralization and inspection of log sources across your entire AWS workload without using Security Lake.\nConfiguration and Change Management Guidance in this category emphasizes centralization, securing, and protecting configuration. The guidance highlights the importance of detecting and alerting on unauthorized modifications, auditing configuration for compliance, and change management processes that automate frequent changes to minimize unintended drift.\nIn AWS, you can deploy infrastructure and configuration as code, enabling centralized configuration storage in repositories, tracking changes through version control, and deploying changes through approved change management processes. You can use code repositories and continuous integration and continuous delivery (CI/CD) processes to automate the deployment of these configurations, helping you accelerate deployment, maintain consistency, simplify management, and implement stringent, auditable change control processes.\nRegardless of how infrastructure is deployed and managed, you can use AWS Config to automatically track the current state and history of a vast set of configuration information across over 100 AWS services and hundreds of their resource types. You can also write custom AWS Config rules to take automated actions whenever sensitive resources are modified, or leverage over 400 AWS-managed rules in AWS Config to send alerts or auto-remediate when critical resources change state.\nIdentity and Access Management Guidance in this category emphasizes the importance of managing operational accounts and permissions, using phish-resistant authentication methods, implementing least privilege through role-based access control, managing emergency access, and session limiting.\nAuthentication and authorization, which are critical components of access control, are managed through AWS Identity and Access Management (IAM), AWS IAM Identity Center, and AWS Organizations. AWS provides you with the ability to manage permissions at scale across identities, resources, and services, including the ability to require multi-factor authentication (MFA) for sign-ins. Furthermore, these capabilities support customer compliance with the principle of least privilege by encouraging the management of time-limited, session-based credentials using AWS Security Token Service (AWS STS).\nSoftware running in the cloud needing to call cloud APIs automatically receives temporary, frequently rotated credentials through IAM roles for Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), and AWS Lambda, eliminating the need for long-term credentials that could be leaked or compromised. Access to cloud APIs from on-premises software can be securely bootstrapped from enterprise identity management technologies using AWS IAM Roles Anywhere. You can even protect authentication for non-cloud technologies by combining roles and using AWS Secrets Manager to protect and automatically rotate secrets like database passwords.\nNetwork and Traffic Management Guidance in this category emphasizes segmenting workloads and networks to limit lateral movement and internet exposure, monitoring and regulating traffic flow using policies, and securing unused ports.\nYou can achieve micro-segmentation of the network, an important aspect of modern security architecture, through VPCs and subnets. For example, you can separate internet-facing components of an application from components that don\u0026rsquo;t require this access by placing them in separate VPCs and only permitting internet access on the VPC that requires it. You can control traffic within and between segments using multiple networking services—for example, route tables, internet gateways, transit gateways, and firewall services. This segmentation minimizes risk from unauthorized activities originating from the internet and limits lateral movement in case of a compromise.\nTo implement guidance for out-of-band management, you can design your network connections to separate management traffic from network signaling traffic using subnets—for example, a single EC2 instance can have multiple elastic network interfaces (ENIs) attached to different subnets or even different VPCs: one interface permitting only management traffic and another permitting only signaling traffic.\nStrong Cryptography for Data-at-Rest and Data-in-Transit Guidance in this category emphasizes the use of strong cipher suites, secure versions of cryptographic protocols, and PKI-based certificates to protect data at rest and in transit.\nEncryption, the cornerstone of data protection, is comprehensively addressed in AWS services. AWS service API endpoints support TLS 1.3 (and a minimum of TLS 1.2) with secure, standards-based cipher suites, cryptographic keys, and advanced security features like HKDF (HMAC-based Extract-and-Expand Key Derivation Function) for enhanced security. AWS services managing customer secrets transmitted over the wire also support post-quantum cryptography. For example, AWS Key Management Service (AWS KMS), AWS Certificate Manager, and AWS Secrets Manager support optional post-quantum key exchange hybrids for the TLS network encryption protocol. When using the Border Gateway Protocol (BGP), AWS uses Resource Public Key Infrastructure (RPKI) and Route Origin Authorizations (ROAs) to protect Amazon\u0026rsquo;s IP address space and routes from misconfiguration and hijacking.\nYou can also use AWS encryption services like AWS KMS, AWS CloudHSM, and AWS Certificate Manager to secure your data in transit and at rest. Keys you create in AWS KMS are protected by Hardware Security Modules (HSMs) validated to FIPS 140-2 Level 3, and there is no mechanism that allows anyone, including AWS service operators, to view, access, or export the plaintext key material.\nAWS Secrets Manager helps you securely manage, retrieve, and rotate database credentials, application credentials, OAuth tokens, API keys, and other secrets throughout their lifecycle. For more details on AWS encryption solutions and best practices, please refer to AWS Encryption Best Practices.\nVulnerability Management This guidance emphasizes mitigating exploitation risk through proper lifecycle management, regular patching, and removal of insecure protocols. AWS helps address these requirements through both shared responsibility and innovative architectural approaches.\nUnder the shared responsibility model, AWS manages the security of the underlying infrastructure. This includes maintaining updated systems and services, disabling insecure protocols and unused ports, and providing a Security Bulletin for timely vulnerability notifications. AWS services are supported under contractually defined terms, so you don\u0026rsquo;t need to worry about expiring infrastructure components.\nFor your applications, AWS enables a transformative approach to vulnerability management through ephemeral resources and immutable infrastructure. Instead of maintaining long-lived instances requiring continuous patching, you can maintain a single, hardened, and regularly updated Amazon Machine Image (AMI) as your golden image. When updates are needed, instead of patching running instances, you simply deploy new instances with your application code installed from the updated AMI. Similar approaches apply to container-based workloads. AWS Lambda-based workloads further minimize your patching responsibility, as you only need to update the code containing your business logic (and any support layers you\u0026rsquo;ve selected)—AWS automatically patches the underlying hypervisor, operating system, and container. This approach allows you to keep your system in a known, safe state while reducing both the threat surface and operational cost. You can further enhance security using AWS networking features like security groups to disable insecure protocols, such as enforcing HTTPS instead of HTTP.\nConclusion The comprehensive guidance from cybersecurity agencies provides an essential framework for securing telecommunications infrastructure. As demonstrated in this article, AWS offers a robust set of services and capabilities that align with the recommendations from CISA and allied governments. From enhanced visibility through logging and monitoring, to strong identity management, network segmentation, encryption, and vulnerability management, AWS provides the tools you need to implement these security controls effectively while maintaining operational performance. The shared responsibility model, combined with AWS\u0026rsquo;s continuous security innovation, enables telecommunications companies to build and maintain secure, resilient cloud environments.\n"},{"uri":"https://ttjendatgit.github.io/trantiendat/en/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Gain Compliance Insights into Your AWS Environment with Amazon Q Business Authors: Jegan Sundarapandian, Neeharika Naidu and Snehal Nahar on 22 MAY 2025 in Amazon Q Business, AWS Config, Customer Solutions, Management \u0026amp; Governance, Management Tools, Technical Guide Permalink\nEnterprise organizations managing multiple AWS accounts face complexity as their cloud infrastructure expands. The exponential growth of resources, coupled with diverse configuration requirements across different business units, creates significant challenges in maintaining effective oversight of AWS environments.\nAWS Config is a service that continuously assesses, audits, and validates the configuration and relationships of your resources on AWS, on-premises, and on other clouds. AWS Config data is stored in secure Amazon S3 buckets. When combined with Amazon Q Business\u0026rsquo;s natural language processing capabilities, this AWS Config data can be used to analyze AWS resource configurations, gather insights, and take action.\nIn this blog post, we\u0026rsquo;ll show how security and compliance teams can now use AWS Config and Amazon Q Business to gain insights into their AWS environments. By leveraging natural language queries, teams can access critical compliance insights and proactively identify potential risks as well as determine remediation actions.\nSolution Overview Our solution addresses this challenge by integrating AWS Config, Amazon S3, and Amazon Q Business to create a natural language interface for querying AWS resource configurations. Here\u0026rsquo;s how it works:\nExtract Relevant AWS Config Data: Our solution periodically extracts AWS Config data from a central Amazon S3 Bucket and copies this data to a secure S3 Bucket in an audit account. This is done for a list of selected AWS Accounts and Regions. Process Data with Amazon Q Business: We then configure Q Business to analyze the AWS Config data stored in the secure S3 bucket in your audit account and create a queryable knowledge base using natural language. Query the Knowledge Base with Natural Language: With the knowledge base generated by Q Business, users can now ask questions in natural language about their AWS environment, such as \u0026ldquo;Which EC2 instances are running in my us-west-2 account?\u0026rdquo; or \u0026ldquo;What is the configuration of my RDS databases in the development environment?\u0026rdquo;. Q Business will then provide relevant information from the underlying AWS Config data. By implementing this solution, your security team members can easily access and understand AWS resource configurations, their compliance status, and identify remediation actions through simple natural language questions.\nSolution Architecture Figure 1: Solution Architecture Diagram Prerequisites In a multi-account AWS environment, audit accounts and log archive accounts are shared AWS accounts used by security and compliance teams. The log archive account contains a central Amazon S3 repository to store copies of all logs, including AWS CloudTrail and AWS Config log files for all other accounts in your AWS Organization.\nThe audit account should be restricted to security and compliance teams, with auditor (read-only) and administrator (full access) roles across all accounts in the AWS Organization. These roles are designed for use by security and compliance teams to conduct audits through AWS mechanisms.\nThis solution is designed to be deployed in an audit account, which can provide a segregated environment to host the AWS Config data needed for Amazon Q Business. We recommend you deploy this solution in a separate audit account from the central log archive account.\nBefore diving into the solution, let\u0026rsquo;s review the prerequisites needed to get started:\nNecessary AWS Identity and Access Management (IAM) access in the log archive and audit account where this solution will be deployed. Access to the AWS Console of the audit account. AWS CLI to deploy necessary artifacts using AWS CloudFormation. SAM CLI – Install SAM CLI. The Serverless Application Model (SAM) CLI is an extension of the AWS CLI that adds functionality for building and testing Lambda applications. Set up Q Business with necessary permissions. Configure an IAM Identity Center instance for the Amazon Q Business application to enable managing end user access to your Amazon Q Business application. How to Build and Deploy the Solution Step 1: Deploy S3 read-only role in the log account hosting the central S3 bucket for AWS Config. Using AWS CLI on the log account, create the ConfigDataReadRole with trust policy.\naws iam create-role \u0026ndash;role-name ConfigDataReadRole \u0026ndash;assume-role-policy-document \u0026lsquo;{ \u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;, \u0026ldquo;Statement\u0026rdquo;: [ { \u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;, \u0026ldquo;Principal\u0026rdquo;: { \u0026ldquo;AWS\u0026rdquo;: \u0026ldquo;arn:aws:iam:::root\u0026rdquo; }, \u0026ldquo;Action\u0026rdquo;: \u0026ldquo;sts:AssumeRole\u0026rdquo;, \u0026ldquo;Condition\u0026rdquo;: {} } ] }\u0026rsquo;\nThen attach the AmazonS3ReadOnlyAccess managed policy.\naws iam attach-role-policy \u0026ndash;role-name ConfigDataReadRole \u0026ndash;policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\nStep 2: Deploy the solution in the audit account\nUsing AWS CLI on the audit account, deploy the solution.\ngit clone https://github.com/aws-samples/sample-Compliance-Insights-Using-Amazon-Q cd sample-Compliance-Insights-using-Amazon-Q sam deploy —guided —capabilities CAPABILITY_NAMED_IAM\nSAM Deployment Parameters: Stack Name: Name of the deployed AWS CloudFormation stack. Example: Stack Name : config-copy-stack\nAWS Region: AWS region where the stack will be deployed. Example: AWS Region : us-east-1\nParameter SourceBucketArn: Bucket ARN for Central AWS Config S3 Bucket in the Log account. Example: Parameter SourceBucketArn: arn:aws:s3:::aws-controltower-logs-123456789101-us-east-1\nParameter AccountList: Provide a comma-separated list of AWS account numbers from which AWS Config data will be extracted and copied to the audit account. Example: Parameter AccountList: 012345678910,123456789101\nParameter RegionList: Provide a comma-separated list of AWS regions from which AWS Config data will be extracted to the audit account. Example: Parameter RegionList : us-east-1,eu-west-1\nParameter SourceAccountId: Provide the AWS account number of your AWS log archive account in your organization, which contains the source AWS Config logs. Example: Parameter SourceAccountId : 123456789101\nOther SAM CLI Inputs\n#Shows you resources changes to be deployed and require a \u0026lsquo;Y\u0026rsquo; to initiate deploy Confirm changes before deploy [Y/n]: Y\n#SAM needs permission to be able to create roles to connect to the resources in your template Allow SAM CLI IAM role creation [Y/n]: Y\n#Preserves the state of previously provisioned resources when an operation fails Disable rollback [Y/n]: Y\nSave arguments to configuration file [Y/n]: Y SAM configuration file [samconfig.toml]: samconfig.toml SAM configuration environment [default]: default\nAfter the SAM deployment completes, run the following SAM command and note the OutputValue of the OutputKey – DestinationBucketName. This S3 Bucket Name will be used as a parameter when deploying the Q Business CloudFormation Template in Step 3.\nsam list stack-outputs \u0026ndash;stack-name config-copy-stack \u0026ndash;output table\nStep 3: Deploy and configure Q Business\nAmazon Q Business is a service that enables you to build intelligent search and analysis applications on top of your enterprise data. In this example, we\u0026rsquo;ll use Q Business to analyze compliance-related data stored in an S3 bucket.\nThe CloudFormation template we\u0026rsquo;ll use provisions the following resources:\nQ Business Application: The main application that will host our compliance analysis experience. Q Business Index: The index that the application will use to quickly search and retrieve relevant data. Q Business Retriever: Connects the application to the index, enabling search and retrieval functionality. Q Business Data Source: Configures the S3 bucket as a data source for the application. The S3 data source will be set up to run a sync task weekly at 8 AM Monday UTC time. Q Business Web Experience: Provides a custom web interface to interact with the Q Business application. IAM Permissions and Policies: Grants necessary permissions for Q Business resources to access the S3 bucket and perform actions within the application. Let\u0026rsquo;s walk through the steps to deploy this Q Business application.\nDeploy the CloudFormation Template\nLog into the AWS Console and navigate to CloudFormation. Click \u0026ldquo;Create stack\u0026rdquo; and select \u0026ldquo;With new resources (standard)\u0026rdquo;. Download the CloudFormation template from Github to deploy Q Business. Select \u0026ldquo;Upload a template file\u0026rdquo; and choose the CloudFormation template you\u0026rsquo;ve been provided. 5.Fill in the required parameters: QBusinessApplicationName: Your Amazon Q Business application name.\nS3BucketName: Name of the S3 bucket containing the compliance data collected earlier in Step 3.\nUseIDC: Set to \u0026ldquo;true\u0026rdquo; if you want to use AWS IAM Identity Center to authenticate users.\nUseIdP: Set to \u0026ldquo;true\u0026rdquo; if you want to use an external Identity Provider (IdP) to authenticate users.\nIdentityCenterArn: ARN of your IAM Identity Center instance (required if UseIDC is \u0026ldquo;true\u0026rdquo;).\nExternalIdPArn: ARN of your external IdP (required if UseIdP is \u0026ldquo;true\u0026rdquo;).\nReview the template and its parameters, then click \u0026ldquo;Next\u0026rdquo; to continue. On the next page, configure any additional stack options if needed, then click \u0026ldquo;Next\u0026rdquo;. Review the stack details and acknowledge any required capabilities, then click \u0026ldquo;Create stack\u0026rdquo; to deploy the resources. The CloudFormation deployment will take a few minutes to complete. Once the stack is in \u0026ldquo;CREATE_COMPLETE\u0026rdquo; status, you can proceed to the next steps.\nAssign Users and Groups After deploying the CloudFormation stack, you\u0026rsquo;ll need to manually assign users and groups to the Q Business application. Here\u0026rsquo;s how:\nIn the AWS Console, navigate to Amazon Q Business. Click on the application name created by the template. In the \u0026ldquo;User access\u0026rdquo; section, select \u0026ldquo;Manage user access\u0026rdquo;. Select \u0026ldquo;Add groups and users\u0026rdquo;, then choose either \u0026ldquo;Add and assign new users\u0026rdquo; or \u0026ldquo;Assign existing users and groups\u0026rdquo;. Provide the group/user name. Select the group and choose \u0026ldquo;Assign\u0026rdquo;. Select the appropriate subscription package (e.g., Q Business Pro). Select \u0026ldquo;Confirm\u0026rdquo; to complete the assignment. Access the Q Business Web Experience You can now access the custom web experience for your Q Business application using the URL output by the CloudFormation template. The web experience provides a user-friendly interface to search, browse, and interact with your compliance data stored in your S3 bucket.\nTest the Solution\nTo test the solution, we\u0026rsquo;ll log into the Amazon Q Business application with credentials and interact using the following questions:\nTo test the solution, we\u0026rsquo;ll log into the Amazon Q Business Application with the credentials of the users who were previously assigned to the Q Business application. Let\u0026rsquo;s interact using the following questions:\nList all non-compliant S3 buckets\nWhen was the group \u0026ldquo;name of group\u0026rdquo; created and when did it become non-compliant\nHow can we remediate the non-compliant bucket \u0026ldquo;name of bucket\u0026rdquo;\nFigure 2: To list non-compliant S3 buckets Figure 3: To learn when an S3 bucket became non-compliant Figure 4: To remediate a non-compliant S3 bucket Cleanup Delete the config-copy-stack application you created, using SAM CLI. sam delete Delete the S3 read-only role in the log account. aws iam delete-role \u0026ndash;role-name ConfigDataReadRole Delete Users and Groups in the Audit Account. Locate and delete the stack created by the CloudFormation template deployment. Conclusion By integrating AWS Config and Amazon Q Business, you can harness the power of natural language processing to gain insights into your AWS environment. This solution enables your team members to easily access and understand their AWS resource configurations. As your cloud infrastructure grows, this solution can help you stay on top of your resource configurations and make informed decisions to optimize your AWS environment. Deploy this AWS Config and Amazon Q Business integration to gain deeper visibility and compliance insights across your entire AWS environment.\n"},{"uri":"https://ttjendatgit.github.io/trantiendat/en/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" title: \u0026ldquo;Blog 3\u0026rdquo; date: \u0026ldquo;2025-09-09\u0026rdquo; weight: 1 chapter: false pre: \u0026quot; 3.3. \u0026quot; Four ways to grant cross-account access in AWS Authors: Anshu Bathla and Jay Goradia on February 24, 2025 in AWS Identity and Access Management (IAM), Best Practices, Intermediate (200), Security, Identity, \u0026amp; Compliance, Technical Guide Permalink\nAs your Amazon Web Services (AWS) environment grows, you may need to grant resource access to multiple accounts. This can stem from various reasons, such as enabling centralized operations across multiple AWS accounts, sharing resources between teams or projects in an organization, or integrating with third-party services. However, granting access to multiple accounts requires careful consideration of security, availability, and manageability requirements.\nIn this blog post, we\u0026rsquo;ll explore four different ways to grant cross-account access using resource-based policies. Each method has its own trade-offs, and the best choice depends on your specific requirements and use case.\nEvaluating different techniques to grant cross-account access\nCross-account access is granted through identity-based policies and resource-based policies in AWS Identity and Access Management (IAM). Identity-based policies are attached to IAM roles, while resource-based policies are attached to resources like Amazon Simple Storage Service (Amazon S3) buckets and AWS Key Management Service (AWS KMS) keys. Resource-based policies require you to specify one or more principals (users or IAM roles) that are allowed to access the resource.\nYour choice of how to define the principal in the resource-based policy will affect several aspects of both the security and availability of the solution. This article focuses on understanding this impact and selecting appropriate trade-offs for your use case.\nAn example scenario\nImagine you have an S3 bucket in an AWS account (Account A) that needs to be accessed by various principals in another AWS account (Account B). In this case, we assume that the principals in Account B have the necessary S3 access in their identity-based policies, and we will focus on creating resource-based policies in Account A. Although the methods explained here use Amazon S3, the concepts discussed apply to all AWS services that support resource-based policies. In the following sections, we\u0026rsquo;ll guide you through four different ways to grant cross-account access in this case and discuss the trade-offs of each method.\nMethod 1: Grant access to a specific IAM role using the Principal element of the resource-based policy\nIn this example, you use a bucket policy to grant access to a specific IAM role (RoleFromAccountB) in Account B by specifying the Amazon Resource Name (ARN) of the IAM role in the Principal element of the policy in Account A.\n{ \u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;, \u0026ldquo;Statement\u0026rdquo;: [ { \u0026ldquo;Sid\u0026rdquo;: \u0026ldquo;AllowRoleInThePrincipalElement\u0026rdquo;, \u0026ldquo;Principal\u0026rdquo;: { \u0026ldquo;AWS\u0026rdquo;: \u0026ldquo;arn:aws:iam::111122223333:role/RoleFromAccountB\u0026rdquo; }, \u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;, \u0026ldquo;Action\u0026rdquo;: \u0026ldquo;s3:GetObject\u0026rdquo;, \u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;arn:aws:s3:::amzn-s3-demo-bucket-account-a/*\u0026rdquo; } ] }\nUsing this bucket policy, if someone in Account B deletes or recreates the role (RoleFromAccountB), that role will no longer be able to access the amzn-s3-demo-bucket-account-a bucket, even if the role is recreated with the same name. The reason is that when you save this policy, the role\u0026rsquo;s ARN is mapped to the role\u0026rsquo;s unique ID, which looks something like this: AROADBQP57FF2AEXAMPLE. You\u0026rsquo;ll see the role identifier in the Principal element of resource-based policies if you view them after deleting the role they referenced.\nThis behavior is intentional. Resource-based policies only allow the specific instance of the role that you set as the principal at the time of policy creation. This helps prevent unintended access to your resource if you delete a role but forget to update the resource-based policy to remove it. This behavior can also pose an availability risk because the role (RoleFromAccountB) will have a new unique ID when recreated and will no longer have access to the bucket. The role might be recreated for several reasons, including accidentally when you use tools like infrastructure as code.\nYou might consider choosing this method if:\nYou own the roles in both Account A and Account B and can control the creation and deletion of these roles.\nYou want the resource-based policy in Account A to stop granting access when the specified role (RoleFromAccountB) is deleted.\nYou prioritize granular access control over potential availability concerns if the role (RoleFromAccountB) is deleted.\nMethod 2: Grant access to an account using the Principal element of the resource-based policy\nIn this example, you grant access to a specific account in the Principal element of the resource-based policy. This resource-based policy from Account A allows any user or role from Account B that has an identity-based policy granting them read object permissions.\nNote: You can use either \u0026ldquo;Principal\u0026rdquo;: {\u0026ldquo;AWS\u0026rdquo;: \u0026ldquo;111122223333\u0026rdquo;} or \u0026ldquo;Principal\u0026rdquo;: {\u0026ldquo;AWS\u0026rdquo;: \u0026ldquo;arn:aws:iam::111122223333:root\u0026rdquo;} in the Principal element. They are equivalent, and the long-form ARN does not represent the root user.\n{ \u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;, \u0026ldquo;Statement\u0026rdquo;: [ { \u0026ldquo;Sid\u0026rdquo;: \u0026ldquo;AllowAccountInThePrincipalElement\u0026rdquo;, \u0026ldquo;Principal\u0026rdquo;: { \u0026ldquo;AWS\u0026rdquo;: \u0026ldquo;111122223333\u0026rdquo; }, \u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;, \u0026ldquo;Action\u0026rdquo;: \u0026ldquo;s3:GetObject\u0026rdquo;, \u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;arn:aws:s3:::amzn-s3-demo-bucket-account-a/*\u0026rdquo; } ] }\nThis resource-based policy helps avoid the potential availability issue discussed in Method 1. If a role in Account B that needs access to the bucket is recreated, it will still have access after the role is recreated. This is because you don\u0026rsquo;t specify the role in the Principal element—instead, you specify an account. If using Method 2, you must be comfortable delegating access control decisions to the owner of that account.\nThis method explicitly delegates access control decisions to IAM in the other account (Account B). Principals in Account B have access to this bucket if they are allowed by their identity-based policies.\nYou might consider choosing this method if:\nYou need to grant access to multiple principals in Account B.\nYou want to delegate access decisions to the account where the owner exists (Account B).\nYou prioritize ease of management and availability over granular access control.\nMethod 3: Grant access to a specific IAM role using the aws:PrincipalArn condition\nThis method builds upon Method 2 and adds a condition that grants access only to a specific IAM role. Similar to Method 2, you use the account number as the value of the Principal element, but also use the aws:PrincipalArn condition key to restrict access to a specific principal in Account B.\nThe aws:PrincipalArn condition key is a global condition key that compares the ARN of the principal making the request with the ARN you specify in the policy. For IAM roles, the request context returns the role\u0026rsquo;s ARN, not the ARN of the user who assumed the role.\n{ \u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;, \u0026ldquo;Statement\u0026rdquo;: [ { \u0026ldquo;Sid\u0026rdquo;: \u0026ldquo;AllowAccountInPrincipalAndRoleInPrincipalArn\u0026rdquo;, \u0026ldquo;Principal\u0026rdquo;: { \u0026ldquo;AWS\u0026rdquo;: \u0026ldquo;111122223333\u0026rdquo; }, \u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;, \u0026ldquo;Action\u0026rdquo;: \u0026ldquo;s3:GetObject\u0026rdquo;, \u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;arn:aws:s3:::amzn-s3-demo-bucket-account-a/*\u0026rdquo;, \u0026ldquo;Condition\u0026rdquo;: { \u0026ldquo;ArnEquals\u0026rdquo;: { \u0026ldquo;aws:PrincipalArn\u0026rdquo;: \u0026ldquo;arn:aws:iam::111122223333:role/RoleFromAccountB\u0026rdquo; } } } ] }\nThis policy comes with similar availability benefits as the policy in Method 2: access to this resource will persist after role recreation. This is because a role is only translated to a unique identifier when used in the Principal element. It is not translated to a unique identifier when used in a condition. If the role (RoleFromAccountB) in Account B is recreated, whether accidentally or intentionally, the policy will continue to grant access because the role matches the role ARN specified in the condition key of the resource-based policy in Account A. Therefore, Method 3 provides a balanced approach between availability and security.\nYou might consider choosing this method if:\nYou are comfortable that this policy will continue to grant access to the role specified in the aws:PrincipalArn condition key if that role (RoleFromAccountB) is recreated.\nYou don\u0026rsquo;t own Account B that you\u0026rsquo;re granting access to and have no control over when the role might be recreated.\nYou want to balance availability with security.\nMethod 4: Grant access to an entire AWS Organizations organization\nThis method focuses on a different use case and is not a replacement for the methods listed above. Use this method if you have a resource (in this example, an S3 bucket) that you want to share with an entire organization, but not with anyone outside.\n{ \u0026ldquo;Version\u0026rdquo;: \u0026ldquo;2012-10-17\u0026rdquo;, \u0026ldquo;Statement\u0026rdquo;: [ { \u0026ldquo;Sid\u0026rdquo;: \u0026ldquo;AllowAccessToAnEntireOrganization\u0026rdquo;, \u0026ldquo;Principal\u0026rdquo;: { \u0026ldquo;AWS\u0026rdquo;: \u0026ldquo;\u0026rdquo; }, \u0026ldquo;Effect\u0026rdquo;: \u0026ldquo;Allow\u0026rdquo;, \u0026ldquo;Action\u0026rdquo;: \u0026ldquo;s3:GetObject\u0026rdquo;, \u0026ldquo;Resource\u0026rdquo;: \u0026ldquo;arn:aws:s3:::amzn-s3-demo-bucket-account-a/\u0026rdquo;, \u0026ldquo;Condition\u0026rdquo;: { \u0026ldquo;StringEquals\u0026rdquo;: { \u0026ldquo;aws:PrincipalOrgId\u0026rdquo;: \u0026ldquo;o-12345\u0026rdquo; }, \u0026ldquo;StringNotEquals\u0026rdquo;: { \u0026ldquo;aws:PrincipalAccount\u0026rdquo;: \u0026ldquo;${aws:ResourceAccount}\u0026rdquo; } } } ] }\nThere is no way to specify an organization using the Principal element of a resource-based policy, so you must use the aws:PrincipalOrgId condition key to restrict access to a specific organization. In this policy, you specify a wildcard in the Principal element, indicating that anyone can access the bucket. Then, the condition reduces \u0026ldquo;anyone\u0026rdquo; to only AWS account principals that belong to the specified organization and have an identity-based policy allowing them access.\nYou then add an additional condition block to compare the aws:PrincipalAccount condition key with the aws:ResourceAccount condition key using a policy variable. This additional condition block is optional and excludes the bucket-owning account (Account A) from the allow statement. The reason to use this additional condition block is so that principals in Account A still need an allow statement in their identity-based policy to access this bucket. If you choose to exclude this aws:PrincipalAccount comparison, principals in Account A will be granted access to the bucket without an explicit allow statement in their identity-based policies. Policy evaluation logic only requires either an identity-based policy or a resource-based policy (but not both) to allow a request when the principal and resource are in the same account.\nYou might consider choosing this method if:\nYou have a shared resource that your entire organization should be able to access.\nConclusion Choosing a method for granting cross-account access requires careful consideration of your requirements and use cases. Each of the four methods discussed in this blog post has its own advantages and disadvantages. By understanding these methods and their implications, you can decide on the most appropriate approach to grant cross-account access to your AWS resources. Remember to regularly review and audit your resource-based policies to ensure they align with your security and access requirements.\n"},{"uri":"https://ttjendatgit.github.io/trantiendat/en/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Tran Tien Dat\nPhone Number: 0339537689\nEmail: dattt1608@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nClass: SE180773\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/09/2025 to 24/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"ONLINE PLATFORM FOR TRACKING AND FORECASTING HURRICANE TRAJECTORY In this workshop, we will present how we created an online platform that allows internet users to freely check, track, and even predict the path of ongoing storms in the West Pacific region. This platform helps users better prepare for upcoming natural disasters and reduces the potential damage they may cause.\nThe platform provides two main functionalities:\nShowing Recent Storms – Allows users to view the path, intensity, wind speed, and other characteristics of recent storms in the West Pacific region. Predicting Hurricane Trajectories – Allows users to input past storm locations (latitude and longitude; at least 9 data points) to obtain predictions about the storm’s near-future movement, intensity changes, and potential path. Following the flow of this workshop, we will discuss the datasets, pre-processing steps, model-training pipeline, and the process of building the online platform using AWS services. We will also demonstrate our proposed augmentation techniques—Stepwise Temporal Fading Augmentation (STFA) and Plausible Geodesic Bearing Augmentation (PGBA)—along with the use of physics-informed machine learning. These approaches enhance the realism of the training data and significantly improve prediction accuracy for storm trajectories, lifetime estimates, and total travel distance.\nFigure 1 : Model pipeline Once the model-training process is completed, we move to building the online platform using a serverless architecture. This architecture is cost-efficient, scalable, and easy to maintain/deploy—making it an ideal choice for our project. Below are the main AWS services used:\nAWS Lambda – Executes the ML models and handles backend logic Amazon S3 – Stores static files, trained models, and storm data Amazon API Gateway – Routes user requests to the appropriate Lambda functions depending on whether they are viewing recent storms or running predictions Amazon CloudFront – Speeds up content delivery through edge locations AWS Secrets Manager – Stores API keys and other sensitive information … – Additional supporting services as needed Figure 2 : Platform Architecture "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Typically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Deepen understanding of AWS networking services, attend AWS Cloud Day Vietnam\nWeek 3: Understand the scaling process for EC2 instances\nWeek 4: Begin foundational learning of Amazon Virtual Private Cloud (VPC) and its core components\nWeek 5: Translate and gain a solid understanding of two important technical blog posts\nWeek 6: Learn to set up Hybrid DNS using Route 53 Resolver\nWeek 7: Learn for the exam\nWeek 8: Prepare and do the exam\nWeek 9: Research and identify suitable AWS services for building a weather API\nWeek 10: Focus on enhancing and optimizing the performance of the weather API\nWeek 11: Focus on serverless and AI/ML topics relevant to project development\nWeek 12: Document all events participated in during the internship and finalize the worklog documentation.\n"},{"uri":"https://ttjendatgit.github.io/trantiendat/en/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Deepen understanding of AWS networking services Learn and practice VPC, Subnets, Route Tables, and security features Attend AWS Cloud Day Vietnam to gain industry insights Continue project planning with the team Tasks to be implemented this week: Day Tasks Start Date End Date Reference Materials Mon - Study Module 02-01: AWS Virtual Private Cloud (VPC)\n- Study Module 02-02: VPC Security and Multi-VPC Features\n- Study Module 02-03: VPN, DirectConnect, Load Balancing concepts\n- Practice: Module 02-Lab03-01: Start with Amazon VPC and AWS\n- AWS Study Group: Module 02-Lab03-01.1 - Subnets\n- AWS Study Group: Module 02-Lab03-01.2 - Route Tables\n- Meet with team to discuss project architecture 15/09/2025 15/09/2025 Tue - Review VPC concepts: CIDR blocks, Internet Gateway, NAT Gateway\n- Study VPC Peering and VPC Endpoints\n- Learn about Network Access Control Lists (NACLs) and Security Groups\n- Practice:\n+ Create custom VPC with public and private subnets\n+ Configure route tables for different subnets\n+ Set up Security Groups for web servers and databases\n- Team meeting: Finalize project requirements and timeline 16/09/2025 16/09/2025 Wed - Prepare for AWS Cloud Day Vietnam\n- Review key questions and topics for the event\n- Study AWS networking best practices\n- Practice:\n+ Implement VPC Flow Logs for monitoring\n+ Configure VPC Peering between two VPCs\n+ Test connectivity between peered VPCs\n- Update project documentation 17/09/2025 17/09/2025 Thu - AWS Cloud Day Vietnam Attendance\n+ Attend keynote sessions on cloud trends\n+ Participate in networking workshops\n+ Learn about AWS new services and features\n+ Network with AWS experts and other attendees\n- Document key learnings from the event\n- Share insights with team members 18/09/2025 18/09/2025 AWS Cloud Day Vietnam Event Materials Fri - Apply Cloud Day learnings to project\n- Study Module 02-04: Advanced VPC configurations\n- Learn about Transit Gateway and AWS PrivateLink\n- Practice:\n+ Implement Load Balancer in VPC\n+ Configure Auto Scaling with VPC\n+ Set up VPN connection to on-premise (simulated)\n- Team meeting: Update project based on Cloud Day insights 19/09/2025 19/09/2025 Week 2 Achievements: Mastered VPC Fundamentals:\nSuccessfully created and configured custom VPCs Implemented public and private subnet architectures Configured route tables for proper traffic routing Set up Internet Gateway and NAT Gateway for internet access Enhanced Security Knowledge:\nConfigured Security Groups for instance-level security Implemented Network ACLs for subnet-level security Understood differences between stateful and stateless firewalls AWS Cloud Day Vietnam Participation:\nGained insights into AWS roadmap and new features Learned industry best practices from AWS experts Networked with cloud professionals and peers Attended hands-on workshops on advanced networking Practical Implementation:\nCreated VPC Peering connections between multiple VPCs Configured VPC Flow Logs for network traffic monitoring Implemented Elastic Load Balancing within VPC environment Set up basic VPN configurations (simulated environment) Project Progress:\nFinalized project architecture design Created detailed network diagram using draw.io with AWS icons Defined clear roles and responsibilities within the team "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/5-workshop/5.2-data-preparation/","title":"Data Preparation","tags":[],"description":"","content":"Data Collection and Preparation Data is a critical component of our project. It not only powers the machine learning model but is also directly displayed to end users so they can monitor the most recent storms in the West Pacific region. Because of this dual purpose—model training and real-time visualization—we carefully investigated multiple reliable and authoritative sources before selecting a single dataset that met all of our requirements: Hurricane Data from NOAA.\nThe National Oceanic and Atmospheric Administration (NOAA) is a scientific agency under the U.S. Department of Commerce. NOAA provides highly accurate, research-grade environmental data, including global weather observations, satellite imagery, and tropical cyclone records. With decades of investment in advanced technologies such as geostationary satellites, ocean buoys, radar systems, and climate monitoring networks, NOAA is widely regarded as one of the most trustworthy providers of hurricane information in the world.\nFor this project, we specifically use data from the International Best Track Archive for Climate Stewardship (IBTrACS)—a NOAA-led project and the world’s most comprehensive tropical cyclone dataset. IBTrACS consolidates and unifies historical and modern storm-track data from multiple meteorological agencies (e.g., JTWC, JMA, CMA, NHC). By merging these sources into a single consistent format, it improves inter-agency comparability and ensures researchers worldwide have access to the best available storm-track information.\nThe version of the dataset we use contains 226,153 rows of hurricane observations. Each row includes a variety of valuable fields such as:\nsid – storm ID number – storm number basin / subbasin – regional classification nature – storm type (e.g., tropical storm, typhoon) iso_time – timestamp lat / lon – storm center coordinates … and many additional meteorological properties However, for our machine learning model, we focus only on four key columns: sid, iso_time, lat, and lon. These form the essential time-series trajectory used to predict storm movement.\nThe dataset spans storms recorded from 1870 up to 2025, filtered to include only those within the West Pacific region—our geographical focus. The raw dataset can be accessed publicly here: https://data.humdata.org/dataset/vnm-ibtracs-tropical-storm-tracks#\nCleaning and Physics-Informed Feature Engineering One advantage of IBTrACS is that it is already well-maintained and consistent. Only minimal preprocessing is needed, primarily the removal of missing values.\nAfter cleaning, we apply our first step of physics-informed machine learning—a technique that injects physical knowledge directly into the data pipeline. From the latitude–longitude coordinates, we compute two additional features using the Haversine formula:\nDistance between consecutive storm points Bearing (direction of movement) These features are physically meaningful: they represent real-world movement patterns instead of arbitrary transformations. They enrich the dataset by giving the model more context about the storm’s momentum and direction, thereby improving learning efficiency and prediction accuracy.\nFigure 1 : Dataset Description Data for Display The data used for display on the platform is different from the data used for training, even though both originate from NOAA. The training dataset is static and historical, but the display dataset must always reflect the current storm conditions.\nTo achieve this, we implement a scheduled AWS Lambda function that automatically retrieves the latest storm-track updates at the end of each day. This ensures that the platform always presents the most recent and accurate information to users.\nThe processed display data is stored as a JSON file in an Amazon S3 bucket. When a user accesses the website:\nThe frontend sends a request to API Gateway API Gateway triggers the appropriate Lambda function The Lambda function fetches the JSON from S3 The resulting data is returned to the user for visualization This pipeline guarantees real-time, serverless, cost-effective data delivery.\nThe live dataset used for display can be accessed here: https://ncics.org/ibtracs/\nFigure 2 : Web to crawl data "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Proposal document Doc\nONLINE PLATFORM FOR TRACKING AND FORECASTING HURRICANE TRAJECTORY Geodesic-Aware Deep Learning for Hurricane Trajectory Prediction: A Physics-Informed and Augmentation-Driven Approach 1. Executive Summary Time-series data serves as one of the most fundamental representations of information in modern scientific and industrial applications. It is essential for understanding dynamic processes such as economic trends, energy consumption patterns, and meteorological changes over time. In particular, weather forecasting heavily relies on time-series data to predict future atmospheric conditions, hurricane trajectories, and seasonal anomalies based on historical records.\nWith the rapid progress in deep learning and neural network research, our project aims to develop an advanced forecasting model capable of accurately predicting the future path, intensity, and total travel distance of moving storms within the next few days. The model’s predictions can support early warning systems, allowing authorities and residents in affected regions to take precautionary measures well before a hurricane reaches their area.\nTo move beyond the limitations of current technologies and existing research, this study introduces several novel techniques and algorithms, including two new augmentation methods for geodesic time-series data and a spatial encoding mechanism designed to enhance the predictive performance of convolutional computing. The final system will be integrated into a cloud-based, serverless architecture on AWS, ensuring scalability, high availability, and cost efficiency for real-time storm tracking and analysis.\nThe system architecture leverages several AWS services to form a fully managed data processing and deployment pipeline. AWS Lambda functions serve as the backbone for serverless computation, automatically triggered by Amazon EventBridge to crawl and process new storm data from open meteorological sources on a scheduled basis. Processed data is stored securely in Amazon S3, while AWS CodePipeline and CodeBuild automate the continuous integration and deployment of new model versions. The trained model is hosted and exposed via Amazon API Gateway, enabling lightweight, real-time inference requests from the online forecasting platform. All system activities are monitored through Amazon CloudWatch, providing operational visibility, fault detection, and performance metrics.\nThe first proposed method, Stepwise Temporal Fading Augmentation (STFA), is a new time-series augmentation framework that models the natural decline in the influence of past observations. Unlike traditional approaches based on random perturbation or noise injection, STFA applies fading weights to earlier time steps while preserving recent information. This process generates realistic and diverse synthetic sequences, improving model robustness and generalization. The technique will be evaluated on hurricane trajectory prediction tasks that rely on sequential latitude–longitude data.\nThe second proposed technique, Plausible Geodesic Bearing Augmentation (PGBA), introduces an augmentation strategy based on the feasible range of storm bearings and distances. By analyzing the geodesic bearing and distance between consecutive storm locations, PGBA defines a realistic motion boundary within which new synthetic trajectories are generated. This approach enhances the model’s capacity to capture natural spatial variability and directional uncertainty in storm movements.\nAdditionally, this study explores a spatial–temporal representation of time-series data, enabling the application of convolutional neural networks (CNNs) to capture both spatial and temporal dependencies. This representation leverages the strengths of convolutional computing to model local interactions across space and time. It will serve as a baseline for comparison with Temporal Convolutional Network (TCN) models trained using the proposed augmentation methods.\nTraditional neural networks, whether used for sequential or image-based modeling, primarily learn statistical patterns from data. However, in many real-world physical systems, such purely data-driven models may fail to adhere to natural constraints, such as gravitational or geodesic relationships. To address this limitation, we incorporate the principles of Physics-Informed Machine Learning (PIML) into our approach. Specifically, geodesic distance and bearing are derived from the latitude–longitude data and integrated into the model’s training process as physically meaningful features. Furthermore, we employ the Haversine formula—which computes the spherical distance between two points—as an auxiliary loss term, complementing standard error metrics such as MSE, RMSE, MAE, and MAPE.\nBy combining the proposed augmentation methods, physics-informed learning principles, and a serverless deep learning infrastructure powered by AWS services, this research aims to develop a scalable, robust, and accurate framework for hurricane trajectory forecasting. The resulting system not only advances the state of geodesic time-series modeling but also demonstrates the practicality of deploying AI-driven environmental prediction systems as resilient, cloud-native applications that enhance preparedness and safety in hurricane-prone regions.\n2. Problem Statement What’s the Problem? To develop a reliable platform for tracking and issuing alerts on future hurricane movements, it is essential to construct a machine learning model that is both accurate and capable of producing dependable predictions. The tracking component can be addressed by continuously collecting and updating data from publicly available meteorological sources. However, these datasets are often geographically constrained and contain redundant or incomplete information.\nIn contrast, the predictive component presents greater complexity. Achieving accurate time-series forecasting in this context typically faces two primary challenges: (1) the limited diversity and coverage of available data, and (2) the absence of physical grounding, which restricts the model’s ability to reflect the underlying geophysical dynamics of hurricane behavior.\nData scarcity: Many time-series forecasting tasks suffer from limited training data. While there are various augmentation methods, few approaches directly focus on the declining importance of past values over time.\nPhysics ignorance: Most neural networks only learn from raw data, without considering real-world physical constraints. In trajectory prediction tasks (e.g., hurricanes), this often leads to unrealistic predictions.\nWe aim to:\nDevelop a new time-series augmentation method (STFA and PGBA) to improve robustness and generalization. Incorporate physics-based constraints into model training, bridging the gap between data-driven learning and real-world dynamics. Testing the strength of convolution network (convol2D) in term of forecasting trajectory Creating an online platform that provide latest information about currents storm and precisely predictions on their trajectory The Solution A - Stepwise Temporal Fading Augumentation STFA generates synthetic time-series sequences by gradually reducing the influence of earlier values. Unlike random noise injection, it systematically applies stepwise fading multipliers across bands of older data.\nLet a univariate sequence be:\n$$ X = [x_0, x_1, \\ldots, x_{T-1}] $$\nwhere $T$ is the sequence length of $X$.\nParameters:\n$n$: number of most steps to remain unchanged. $S$: number of step-bands to apply fading, each band is assigned a constant multiplier. $L = T - n$: length of the fading region. $k = \\frac{L}{S}$: values per band. $I_b$: index set of the $b$-th band. \\[ I_b = {\\ {i \\mid L - b \\cdot k ;\\leq; i ;\\leq; L - (b-1)\\cdot k - 1} ,} \\]\nTransformation:\nWe denote the augmented series as:\n$$ X = [x_0, \\ldots, x_{T-1}] $$\nwith the transformation rules:\n$$ x_t = \\begin{cases} x_t, \u0026amp; t \\in {T-n, \\ldots, T-1}, \\\\ m_b , x_t, \u0026amp; t \\in I_b, \\\\ m_{S+1} , x_t, \u0026amp; t \u0026lt; \\min(I_S), \\end{cases} $$\nwhere multipliers $m_b \\in (0,1)$ decrease monotonically from recent to older bands.\nThis formulation preserves the fidelity of recent history while exerting stronger control on the long-range influence of the sequence. The augmentation forces the model to focus on robust patterns beyond the raw data, while increasing diversity according to the chosen parameters.\nB - Plausible Geodesic Bearing Augmentation The Plausible Geodesic Bearing Augmentation (PGBA) technique enhances the realism and control of synthetic trajectory generation in geospatial time-series forecasting tasks. Unlike conventional random perturbation methods, PGBA introduces stochasticity that remains plausible within the physical constraints of the underlying motion. The generated trajectories are derived from the geometric relationships of past observations rather than from purely random steps, resulting in smoother paths and meaningful variability in the training dataset. This technique apply on every four locations in a data sequence.\nPGBA serves as a complementary augmentation to STFA, enriching the diversity of training samples while preserving the original dynamical structure. Its objective is to create redundant yet physically consistent trajectories that capture potential variations in storm movements or similar geospatial phenomena.\nCore Mechanism\nConsider a storm trajectory represented by a sequence of $n$ ordered geographical points:\n$$ P = [P_1, P_2, \\ldots, P_n] $$\nWe split this sequence into small blocks of 4 points, with $P_i$ as the starting point of each block, defined by its latitude and longitude:\n$$ P_i = (\\phi_i, \\lambda_i) $$\nHere, $\\phi_i$ and $\\lambda_i$ denote latitude and longitude in radians, respectively.\nThe geodesic distance $d_i$ and bearing $\\theta_i$ between consecutive points $P_i$ and $P_{i+1}$ are defined as:\n$$ d_i = \\text{Distance}(P_i, P_{i+1}), \\qquad \\theta_i = \\text{Bearing}(P_i, P_{i+1}) $$\nTo introduce plausible variability, PGBA perturbs the bearing by adding a small, uniformly distributed random noise $\\epsilon_i$:\n$$ \\theta_i^{\\text{aug}} = \\theta_i + \\epsilon_i, \\qquad \\epsilon_i \\sim \\text{Uniform}(-\\delta, \\delta) $$\nwhere $\\delta$ is a tunable angular bound controlling the range of deviation.\nThe first two points always remain unchanged and are used to compute distance and bearing. The subsequent augmented point is then computed using the geodesic destination formula, keeping the distance constant while allowing the bearing to vary within the range of the random noise:\n$$ P_{i+2}^{\\text{aug}} = \\text{Destination}(P_i, d_i, \\theta_i^{\\text{aug}}) $$\nThis process preserves the inter-point distance $d_i$ while slightly perturbing the direction to produce physically plausible deviations.\nMulti-Step Smoothing and Correction\nTo enhance spatial smoothness and generate curvilinear trajectories, PGBA applies a secondary correction at every fourth point. Let $P_{i+3}^{\\text{aug}}$ denote the fourth point. It is recomputed such that its bearing $\\theta_{i+3}^{\\text{corr}}$ minimizes the deviation from the original point $P_{i+3}$:\n$$ \\theta_{i+3}^{\\text{corr}} = \\arg\\min_{\\theta}, \\text{Distance}\\Big( \\text{Destination}(P_{i+2}^{\\text{aug}}, d_{i+2}, \\theta),; P_{i+3} \\Big) $$\nThen, the corrected augmented point is obtained as:\n$$ P_{i+3}^{\\text{aug}} = \\text{Destination}(P_{i+2}^{\\text{aug}}, d_{i+2}, \\theta_{i+3}^{\\text{corr}}) $$\nThis step ensures smooth transitions across multiple points while maintaining physical plausibility.\nNote that the first two and last locations in every geodesic time-series sequence always remain unchanged.\nC - Physics-Informed Machine Learning Neural network models such as RNNs, CNNs, and Transformers do not require explicit formulas or task-specific rules to perform well, provided that they are trained with sufficient data. For example, in machine translation tasks such as German-to-English translation using an RNN, no explicit grammar rules are provided during training. Nevertheless, the model is capable of producing coherent translations, which demonstrates one of the major strengths of deep learning: the ability to learn complex patterns directly from data. In contrast, traditional approaches—such as early versions of rule-based translation systems (e.g., Google Translate prior to the 2000s)—relied heavily on grammar rules and dictionaries. While precise, such systems often lacked flexibility and failed when encountering words with multiple meanings or when handling context-dependent structures.\nInspired by this, our goal is to combine the strengths of deep learning with human-defined formulas in order to achieve better performance. Specificly in this geography field, we will try to add the benefit from Haversine formula into training for distance and beering calculation between two location on a sphere. These provide the model with additional structure and inductive bias, guiding learning beyond purely statistical correlations.\nHaversine Formula\nFor distance calculation\nThe Haversine formula is used to calculate the great-circle distance between two points on the surface of a sphere — that is, the shortest path over the Earth’s surface.\n$$ d = 2r , \\arcsin!\\left( \\sqrt{ \\sin^2!\\left(\\frac{\\Delta \\varphi}{2}\\right) + \\cos(\\varphi_1)\\cos(\\varphi_2) \\sin^2!\\left(\\frac{\\Delta \\lambda}{2}\\right) } \\right) $$\nWhere:\n$\\varphi_1, \\lambda_1$ and $\\varphi_2, \\lambda_2$ are the latitudes and longitudes of the two points (in radians). $\\Delta \\varphi = \\varphi_2 - \\varphi_1$ $\\Delta \\lambda = \\lambda_2 - \\lambda_1$ $r$ is the Earth’s radius (≈ 6,371 km). In our framework, instead of relying solely on standard loss functions such as MSE, RMSE, or MAPE, we propose using the Haversine formula for distance calculation as the primary loss function. As the model outputs latitude and longitude coordinates for the next hurricane location, the Haversine formula directly measures the distance between predicted and ground-truth points. A distance close to 0 indicates a highly accurate prediction, while a large distance signals a significant error.\nFor beering calculation\nThe Bearing calculation is dereived from Haversine Formula gives the direction from one geographic point to another along the great-circle path:\n$$\\theta = \\text{atan2}!\\left(\\sin(\\Delta \\lambda)\\cos(\\varphi_2),, \\cos(\\varphi_1)\\sin(\\varphi_2) - \\sin(\\varphi_1)\\cos(\\varphi_2)\\cos(\\Delta \\lambda)\\right)$$\nWhere:\n$(\\varphi_1, \\lambda_1)$ is the start point. $(\\varphi_2, \\lambda_2)$ is the end point. $\\Delta \\lambda$ is the difference in longitude. The result $\\theta$ represents the initial bearing (azimuth) measured clockwise from true north.\nIn our implementation, we fully exploit the use of Haversine Formula to compute two additional features — “distance” and “bearing” — which are appended to the dataset.\nThese features provide the model with richer information about hurricane trajectories while maintaining the core objective of predicting the next geographic location.\nD - Short overview of misconceptions in Common Approaches to Sequence Modeling In the field of sequence modeling within deep learning, recurrent architectures such as RNNs, LSTMs, and GRUs are often considered the default solutions. This perception has led many practitioners and researchers to overlook alternative architectures, particularly convolutional neural networks (CNNs), which are traditionally associated with image processing tasks. Textbooks and courses frequently categorize tasks such as language modeling, translation, or other sequential predictions as the domain of recurrent networks, while convolutional networks are presented primarily in the context of spatial data like images. As a result, the potential of CNNs for sequence modeling is frequently underestimated or ignored.\nConvolutional networks offer several intrinsic advantages that make them well-suited for sequential data. Their inherent parallelism allows for significantly faster training compared to strictly sequential models. Additionally, CNNs are highly effective at capturing local spatial and temporal dependencies, a property that can be leveraged in time-series forecasting and other sequential tasks. Despite these benefits, CNNs are often misunderstood as being unsuitable for sequences due to their lack of explicit memory mechanisms and the absence of intrinsic temporal ordering.\nIn our study, we focus on hurricane trajectory prediction, where the data consists of time-series records of latitude and longitude coordinates. We demonstrate that this type of sequential data can be effectively modeled using CNNs, leveraging their computational efficiency and ability to capture local spatiotemporal patterns. To facilitate this, we encode the locations into a 2D matrix representation, enabling the convolutional network to more effectively extract and learn patterns from the data. Each entry in the matrix corresponds to a “pixel” of an image, an approach we refer to as Trajectory-as-Image. Our methodology employs a standard CNN as a baseline, which is then compared with more specialized sequence models, including TCNs, LSTMs, and RNNs, while incorporating various data augmentation techniques, including two novel methods proposed in this work.\nThrough systematic experimentation and evaluation, we aim to challenge the prevailing notion that convolutional architectures are ill-suited for sequential data. By highlighting the effectiveness of CNNs in sequence modeling, we hope to broaden the perspective of researchers and practitioners, encouraging them to explore convolutional computing as a viable and competitive approach in time-series forecasting and other sequential prediction tasks.\nBenefits and Return on Investment Performance Boost: STFA + PGBA generates structured synthetic sequences that enhance model robustness, reduce overfitting, and improve generalization on unseen storm trajectories.\nPhysics Awareness: Incorporating geographical principles such as distance and bearing increases interpretability and ensures physically consistent predictions.\nNew Research Direction: Establishes two novel paradigms for time-series augmentation based on temporal relevance fading, expanding the methodological toolkit for sequence learning.\nScalability and Reusability: The combined STFA + PGBA + PIML framework can be extended to other sequential forecasting domains such as energy demand, traffic flow, and financial trends.\nOverall Impact: By improving predictive stability and interpretability while maintaining scalability, the proposed approach delivers both scientific value and practical return on computational investment.\n3. Solution Architecture The online platform provides users with up-to-date information on recent storms and a powerful tool for hurricane trajectory predictions. Visitors can either view recent storm data or run predictions using the ML models. The results are displayed interactively on a map, showing storm location, time, and predicted path.\nThe platform is built using a serverless AWS architecture to reduce operational costs while maintaining scalability and reliability. Frontend content is hosted on Amazon S3 and delivered globally via CloudFront, ensuring low-latency access. Users’ requests are routed through API Gateway to Lambda functions, which handle prediction computations and data retrieval. Pre-trained ML models and recent storm datasets are securely stored in S3, with weekly updates managed automatically by EventBridge-triggered crawler Lambdas. Sensitive API keys are stored in Secrets Manager, and system performance is monitored through CloudWatch logs and metrics. IAM enforces least-privilege access for all services.\nThe predictive models leverage the proposed STFA (Stepwise Temporal Fading Augmentation) and PGBA (Plausible Geodesic Bearing Augmentation) techniques. These methods generate realistic synthetic time-series trajectories, preserving temporal relevance and spatial consistency, which significantly improves the robustness and accuracy of the models. By integrating STFA and PGBA, the platform delivers more precise hurricane path predictions, helping users better understand storm behavior and make informed decisions.\nThis architecture enables a responsive, cost-efficient, and secure platform where users can visualize real-time storm information and explore hurricane trajectory forecasts with interactive maps, supported by advanced augmentation methods to boost model performance.\nFigure 1 : Model pipeline Figure 2 : Platform Architecture AWS Services Used Amazon S3: Stores static frontend files, pre-trained ML models, and recent storm data. AWS Lambda: Runs prediction models, fetches storm data, and executes web crawling automation. Amazon API Gateway: Handles frontend requests for predictions and storm data. Amazon CloudFront: Delivers static content globally with low latency. Amazon Route 53: Routes user traffic to CloudFront. Amazon EventBridge: Schedules weekly data crawling. AWS Secrets Manager: Stores external API keys securely. Amazon CloudWatch: Monitors Lambda logs, performance metrics, and system health. AWS IAM: Assigns least-privilege access to all services. Component Design Frontend Layer: Hosted on S3 and delivered through CloudFront. Backend Layer: Handles prediction and data retrieval using API Gateway and Lambda. Data Storage: ML models stored in S3, recent storm data updated weekly by the crawler. Automation: EventBridge triggers a weekly Crawler Lambda to fetch external storm data. Security \u0026amp; Monitoring: Secrets stored in Secrets Manager, metrics and logs collected via CloudWatch. 4. Technical Implementation Implementation Phases This project has three main parts: building the prediction pipeline, setting up data crawling, and deploying the web platform. Each part follows four phases:\nDesign Architecture: Plan AWS serverless stack, Lambda functions, S3 structure. (Weak 1) Estimate Costs: Use AWS Pricing Calculator to assess feasibility and adjust design (Weak 1-2). Optimize Architecture: Fine-tune Lambda memory, S3 usage, and caching to reduce cost (Weak 2-4). Develop, Test, Deploy: Implement Lambda functions, event scheduling, ML model integration, and web frontend with Next.js (Weak 4-8). Technical Requirements\nML Models: Pre-trained trajectory models stored in S3 (.h5/.pth), loaded by prediction Lambda. Storm Data: Weekly updated JSON files, stored in S3, used for frontend display and prediction validation. Serverless Infrastructure: Lambda for prediction, fetching, and crawling; API Gateway for frontend requests; CloudFront/S3 for content delivery. Security: Secrets Manager for API keys, IAM for least-privilege access. Monitoring: CloudWatch for logging and Lambda Insights metrics. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Weak 1): Planning, research on external weather APIs, and ML model preparation.\nInternship (Weak 1-8):\nWeak 1-2: AWS study, architecture design, and cost estimation. Weak 2-4: Optimize architecture, configure serverless workflow, and integrate ML models. Weak 4-8: Implement Lambda functions, set up frontend, test system, and deploy to production. Post-Launch: Continuous data collection and monitoring for up to 1 year.\n6. Budget Estimation Region: ap-southeast-1 (Singapore)\nThe estimated monthly costs for running the hurricane prediction platform on AWS are as follows:\nA. Frontend \u0026amp; Content Delivery\nAmazon S3 (Static Files): Hosts 5 GB of frontend files (HTML, CSS, JS) and handles 10 GB of data transfer per month. Cost ≈ $0.54/month. Amazon CloudFront: Handles 50 GB of data transfer and up to 1 million requests (within free tier). Cost ≈ $6.00/month. Amazon Route 53: 1 hosted zone and 1 million DNS queries per month. Cost ≈ $0.90/month. AWS Certificate Manager (ACM): Provides TLS certificate for secure HTTPS access. Free of charge. Subtotal for Frontend \u0026amp; CDN: ≈ $7.4/month\nB. Backend (API \u0026amp; ML Processing)\nAmazon API Gateway: Handles 1 million HTTP API requests per month, each request approximately 1 MB in size. Cost ≈ $2.5/month. Lambda (Storm Prediction): Predicts hurricane trajectories using ML models. Runs ~1,000 times per day with 512 MB memory allocated and 1 GB ephemeral storage. Each execution lasts ~5 seconds. Cost ≈ $2.54/month. Lambda (Fetch Recent Storm Data): Fetches storm data from S3 for the frontend. Runs ~20,000 times per day with 512 MB memory and 512 MB ephemeral storage for 1 second per execution. Cost ≈ $0.00/month (covered by free tier). Subtotal for Backend: ≈ $4.54/month\nC. Automation \u0026amp; Data Crawling\nAmazon EventBridge: Schedules weekly crawling of storm data (1 cron trigger per day). Free under AWS free tier. Lambda (Web Crawler): Fetches data from external APIs weekly. Uses 128 MB memory and 512 MB ephemeral storage, ~30 seconds per execution. Cost ≈ $0.00/month (free tier). AWS Secrets Manager: Stores 5 API keys for secure access to external weather services. Cost ≈ $2.00/month. Subtotal for Automation \u0026amp; Data Crawling: ≈ $2.0/month\nD. Monitoring \u0026amp; Logging\nAmazon CloudWatch Logs: Collects logs from all Lambda functions and delivers to S3 with 1-month retention. Approximately 2 GB of logs per month. Cost ≈ $0.57/month. CloudWatch Metrics (Lambda Insights): Monitors 8 metrics across Lambda functions. First 10 metrics are free, and only record for predictions and crawl data. Cost ≈ $0.00/month. Subtotal for Monitoring \u0026amp; Logging: ≈ $0.57/month\nE. Storage \u0026amp; Data Transfer\nS3 (Model Bucket): Stores ML models (~1 GB) and handles ~60,000 GET requests per month. Cost ≈ $0.05/month. S3 (Recent Storms Bucket): Stores recent storm data (~1 GB) with ~60,000 GET requests and 30 PUT requests per month. Cost ≈ $0.27/month. F. Tooling \u0026amp; Migration\nAWS CodePipeline and CodeBuild for 10 minutes fixing ≈ $0.90/month. Subtotal for Storage \u0026amp; Data Transfer: ≈ $0.32/month\nTotal Estimated Monthly Cost\nFrontend \u0026amp; CDN: $7.4 Backend (API + ML): $4.54 Automation (Crawler + Secrets): $2.0 Monitoring \u0026amp; Logging: $0.57 Storage \u0026amp; Data Transfer: $0.32 Tooling \u0026amp; Migration: $0.9 TOTAL ≈ $15.73/month\n7. Risk Assessment Risk Matrix\nNetwork Outages: Medium impact, medium probability. Data Source Unavailability: Medium impact, low probability. ML Model Errors: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies\nNetwork: Cache recent storm data in S3 to allow frontend display during outages. Data Sources: Store historical storm data for fallback. ML Model: Regular model validation and testing. Cost: Monitor AWS usage and set budget alerts. Contingency Plans\nSwitch to manual updates if external API fails. Rollback to previous ML model using S3 versioning if new model fails. 8. Expected Outcomes Technical Improvements:\nReal-time hurricane trajectory predictions with visualized paths. Scalable serverless system capable of handling thousands of requests/day. Long-term Value:\nCentralized hurricane data for research and analysis. Framework reusable for other geospatial prediction tasks. Low monthly operational cost (\u0026lt; $20/month). "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/4-eventparticipated/4.2-event2/","title":"AWS Cloud Mastery Series #2","tags":[],"description":"","content":"Summary Report: “DevOps on AWS” Event Objectives Introduce core DevOps principles and the cultural mindset driving modern software delivery Demonstrate how to build automated CI/CD pipelines using AWS services Provide guidance on implementing Infrastructure as Code (IaC) Compare AWS container platforms for deploying cloud-native applications Share best practices for monitoring, observability, and operational visibility Speakers Bao Huynh – AWS Community Builder Thinh Nguyen – AWS Community Builder Vi Tran – AWS Community Builder Key Highlights Understanding the DevOps Mindset Strong collaboration between development and operations enables faster releases Automation reduces repetitive work and improves consistency Continuous feedback loops lead to more stable and resilient systems CI/CD Pipeline on AWS A complete automated pipeline was demonstrated across four stages:\nSource Control: CodeCommit for storing and versioning code Build \u0026amp; Test: CodeBuild for compiling, testing, and packaging Deployment: CodeDeploy supporting rolling, canary, and blue/green strategies Orchestration: CodePipeline connecting and automating each stage Live demos showcased how code commits triggered builds, tests, deployments, and even automated rollbacks.\nInfrastructure as Code (IaC) Transitioning from manual configuration to consistent, versioned infrastructure.\nAWS CloudFormation\nDeclarative YAML/JSON templates Supports parameters, conditions, outputs, and resource definitions Drift detection ensures real-world infrastructure matches the template AWS CDK (Cloud Development Kit)\nCreate infrastructure using TypeScript, Python, Java, and more L1/L2/L3 constructs provide reusable and opinionated patterns CLI supports synth, diff, and deploy Examples showed how identical architectures can be reproduced with IaC instead of traditional “ClickOps.”\nContainers on AWS Introduction to Docker fundamentals and AWS container compute options:\nAmazon ECR: Secure container registry with vulnerability scanning Amazon ECS: AWS-native container orchestration with EC2 or Fargate Amazon EKS: Managed Kubernetes for standardized workloads AWS App Runner: Simplified container hosting with minimal operations The comparison outlined which service fits best depending on skill level, scalability needs, and application patterns.\nObservability and Monitoring Essential practices for ensuring application health:\nAmazon CloudWatch\nMetrics, logs, dashboards, and alarms AWS X-Ray\nDistributed tracing across microservices to detect latency and bottlenecks Emphasis was placed on building useful dashboards, actionable alerts, and proactive monitoring strategies.\nKey Takeaways DevOps Practices Automation increases speed and reliability Alignment between dev and ops teams is crucial Use DORA metrics for continuous improvement Incorporate feedback loops throughout development Infrastructure as Code Reduce manual configuration in production environments CloudFormation offers strong declarative control CDK enables flexible, programmatic definitions Treat infrastructure like software: test, version, automate Application Delivery CI/CD minimizes human error and accelerates releases Choose deployment strategies based on risk tolerance Automated testing should be integrated into every pipeline stage Container Strategy Containers improve portability, consistency, and modularity ECS → straightforward operations model EKS → Kubernetes ecosystem and flexibility App Runner → low-operations approach ECR provides the central repository for container images Observability Combine logs, metrics, and traces for full visibility CloudWatch dashboards + X-Ray service maps simplify troubleshooting Build proactive alerting instead of reacting to failures Applying to Work Automate CI/CD using CodePipeline, CodeBuild, and CodeDeploy Implement IaC with CloudFormation or CDK for repeatable environments Containerize applications and choose ECS, EKS, or App Runner based on project needs Enhance observability with CloudWatch metrics, logs, dashboards, and alarms Use AWS X-Ray for tracing and debugging distributed systems Adopt DORA metrics to measure delivery performance and guide DevOps improvements Event Experience Attending “Cloud Mastery Series #2 – DevOps on AWS” provided both strategic insights and practical knowledge for applying DevOps effectively in cloud environments.\nLearning from highly skilled speakers Clear and detailed explanations of CI/CD, containers, IaC, and monitoring Real-world examples showing how DevOps operates in live production systems Hands-on technical exposure Observed end-to-end CI/CD from commit → build → deployment Learned how CloudFormation and CDK ensure consistent infrastructure Understood trade-offs across ECS, EKS, and App Runner Leveraging modern tools IaC ensures consistency and eliminates configuration drift CloudWatch and X-Ray form the foundation of operational excellence Networking and discussions Opportunities to engage with experts and peers Discussions underscored the importance of culture, automation, and continuous improvement Lessons learned Automation is crucial for safe scaling Observability is essential for reliability Selecting the right container platform reduces operational load Some event photos Figure 1 Figure 2 Figure 3 Overall, the workshop provided a clear and practical understanding of DevOps culture, CI/CD workflows, IaC, container deployment, and observability — all critical components of modern cloud-native development.\n"},{"uri":"https://ttjendatgit.github.io/trantiendat/en/5-workshop/5.3-ml-model/","title":"Machine Learning Model","tags":[],"description":"","content":"MODEL TRAINING PROCESS This work presents the development of a predictive model designed to estimate a storm’s next geographical position using historical observational data from its previous path. To put it simply, we use a sequence of past latitude and longitude values to predict the next latitude and longitude in the future.\nFeature Engineering After completing the data preprocessing stage, we split the dataset into 70% training, 10% validation, and 20% testing. This is done by storm ID, ensuring that no storm appearing in the validation or test set is included in the training set. This prevents data leakage and ensures the reliability of evaluation results.\nFor model training, each input consists of a sequence of 4 consecutive time steps, where each step represents an observation interval of 3 hours. Thus, one input sequence covers a total of 9 hours of storm movement.\nFigure 1 : Dataset Distribution Applying Stepwise Temporal Fading Augmentation (STFA) To enhance the diversity of the training data, we apply our proposed method—Stepwise Temporal Fading Augmentation (STFA)—to 50% of the training set, selected based on unique storm IDs. The original sequences from these storms are replaced by their augmented counterparts, ensuring the final size of the training set remains unchanged (approximately 100% of the original size).\nAs introduced earlier in the proposal section, STFA modifies the older points in a sequence while keeping the most recent observations unchanged. For each 4-step sequence:\nThe latest 2 time steps remain the same The older 2 time steps are scaled using fading coefficients: [0.98,; 0.99] Although these values appear small, latitude and longitude are extremely sensitive features. Even a tiny change—such as from 6.7 to 6.8—can correspond to tens of kilometers of displacement in the real world. Therefore, using modest fading values is both logical and physically meaningful, ensuring the augmented data remains realistic.\nExample of STFA on a 4-Step Sequence Here is a simple example showing how a 4-step time-series sequence is transformed after applying STFA:\nRow Original (lat, lon) Augmented (lat, lon) Operation 1 [-6.8 , 107.5] [-6.66 , 105.35] multiplied by 0.98 2 [-7.0 , 107.1] [-6.93 , 106.03] multiplied by 0.99 3 [-7.3 , 106.7] [-7.3 , 106.7] unchanged 4 [-7.5 , 106.4] [-7.5 , 106.4] unchanged This process reduces the magnitude of older observations while keeping the recent ones intact. The augmentation introduces controlled variability, helping the model generalize better for trajectory forecasting.\nEarlier, we used physics-informed machine learning to compute distance and bearing between time steps using the Haversine formula. After STFA is applied, these values are recomputed based on the augmented coordinates. This guarantees that all physical features remain accurate and consistent with the updated trajectory.\nFigure 2 : Comparison of Augmentation Techniques on Storm Trajectories Model Setting 1.Physics-Informed Loss Our use of the Haversine formula does not end at feature engineering. In addition to generating distance and bearing values, we also incorporate the Haversine distance as a custom loss function, alongside traditional losses such as MSE, RMSE, and MAPE.\nBecause the Haversine formula computes the true geodesic distance between two geographical points, it serves as a natural metric for evaluating the error in the model’s predicted storm location. A larger Haversine distance indicates that the prediction is far from the true position, whereas a smaller value means the model is performing well. The optimal Haversine loss, ideally, is 0 km.\nThe formula was previously introduced in Section 2: Proposal, so we do not rewrite it here.\nExample:\nModel prediction: [-6.72, 107.1] True location: [-6.8, 107.5] Haversine loss: 45.06 km The value 45.06 km directly reflects the real-world positional error, making the loss physically interpretable.\nThis is what we refer to as physics-informed machine learning loss, where physical laws and domain knowledge guide the model’s optimization.\n2.Model Architect Sequence modeling tasks are traditionally handled by recurrent architectures such as RNNs, LSTMs, or GRUs. However, recent research has shown that convolution-based approaches can outperform these methods in many time-series applications.\nWhile CNNs do not inherently model sequence order, they excel at:\nextracting local spatial/temporal features parallel processing faster training stable gradients efficient scaling For our project, we adopt a convolution-based architecture as the foundation. Specifically, our main model is a Temporal Convolutional Network (TCN).\nTemporal Convolutional Network (TCN) A TCN uses 1D dilated convolutions, enabling the model to have a receptive field that expands over time, allowing it to “see” far into the past without requiring recurrence.\nExample (sequence = [a, b, c, d]):\nLayer 1 (dilation = 1): model sees [d] Layer 2 (dilation = 2): model sees [b, d] Layer 3 (dilation = 4): model can see [a, b, c, d] By stacking dilated convolutions, the network learns long-range dependencies while retaining all the benefits of fast convolution operations. Thus, TCNs combine memory of the past with computational efficiency, making them an ideal choice for storm trajectory forecasting.\n3. Model Hyperparameters Below are the key hyperparameters used in our training configuration:\nInput dimensions: 4 (latitude, longitude, distance, bearing) Hidden units: 1024 Number of TCN layers: 2 Learning rate: 1e-4 Epochs: 80 Optimizer: Adam Early stopping: patience = 6 (based on overall loss) 4. Composite Loss Function Our main training loss is a weighted combination of multiple components:\nMSE on latitude and longitude MSE on distance and bearing (auxiliary features) Haversine loss (physics-informed component) The contribution of each auxiliary loss is controlled by weighting factors:\nλ_aux = 0.5 (for distance and bearing MSE) λ_hav = 0.3 (for the Haversine loss) This design ensures that the model:\nlearns to minimize coordinate errors, respects physical displacement, and does not overfit to any single feature. By integrating physics-informed loss terms with data-driven learning, the model becomes more stable, consistent, and aligned with real-world storm dynamics.\nFigure 3 : Training Process Evaluation After the training process concludes and the model triggers early stopping, performing an evaluation on the test set is necessary to determine whether the model generalizes well and is ready for real-world deployment. We evaluate the model using Overall Loss, MSE, RMSE, MAPE, and Haversine distance to gain deeper insight into its performance:\nTotal Loss: 74.3849 MSE: 0.0832 RMSE: 0.2772 MAPE: 0.60% Haversine (km): 30.75 From these results, we observe that the average positional error is approximately 30 km from the true location. This level of error is acceptable because hurricanes are extremely large systems, often spanning hundreds to thousands of kilometers. The MSE of only 0.08 for latitude and longitude also indicates strong predictive accuracy, suggesting that the model can effectively estimate realistic future storm trajectories with minimal error.\nThese results further demonstrate that convolutional computations can perform very well on sequence-modeling tasks, and should be considered a strong alternative to traditional sequential architectures such as RNNs, LSTMs, or GRUs—not only for images but also for structured spatiotemporal data.\nWith the model validated, the next step is to upload the trained model to an Amazon S3 bucket and use an AWS Lambda function to load and execute it in response to user inference requests.\nThe following sections will focus on how we design and deploy our online prediction platform, making the hurricane trajectory model accessible for public use.\nFigure 4 : Evaluation Metrics "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Enhancing Telecommunications Security with AWS AWS shares how to implement CISA\u0026rsquo;s security recommendations for telecommunications infrastructure on the cloud platform, focusing on enhanced visibility and system hardening through native security services.\nBlog 2 - Gain Compliance Insights with Amazon Q Business A guide to integrating AWS Config and Amazon Q Business to create a natural language interface, allowing security teams to easily query and analyze the compliance status of AWS resources.\nBlog 3 - Four Ways to Grant Cross-Account Access in AWS Analyzes four methods for granting cross-account access in AWS using resource-based policies, comparing the security and availability trade-offs of each approach.\n"},{"uri":"https://ttjendatgit.github.io/trantiendat/en/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn key concepts about Amazon Lightsail. Understand the scaling process for EC2 instances. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Brainstorm ideas to enhance the project, identify additional weather data APIs, and implement them. 22/09/2025 22/09/2025 3 - Learn about Amazon Lightsail Workshop and related information. - Practice using service features. 23/09/2025 23/09/2025 https://000045.awsstudygroup.com/ 4 - Explore the functionalities of Amazon Lightsail Containers. - Create and test container instances. 24/09/2025 24/09/2025 https://000046.awsstudygroup.com/ 5 - Study EC2 Auto Scaling concepts: + Manual Scaling + Dynamic Scaling + Scheduled Scaling + Predictive Scaling 25/09/2025 25/09/2025 https://000006.awsstudygroup.com/ 6 - Translate blog titled \u0026ldquo;Enhancing Telecommunications Security with AWS\u0026rdquo; 26/09/2025 26/09/2025 Week 3 Achievements: Amazon Lightsail Learning\nCompleted the Amazon Lightsail workshop and gained a solid understanding of the service\u0026rsquo;s fundamental concepts and information. Practiced and became proficient in key Lightsail features: creating instances, managing storage, networking, and monitoring. Hands-on with Amazon Lightsail Containers\nSuccessfully explored the functionalities of Amazon Lightsail Containers. Created and tested container instances, deploying a sample application onto the container service. EC2 Auto Scaling Study\nGained in-depth knowledge of the four main EC2 Auto Scaling concepts: Manual Scaling: Manually adjusting the number of instances. Dynamic Scaling: Automatically scaling out/in based on CloudWatch metrics (CPU, Network In/Out, etc.). Scheduled Scaling: Automatically scaling based on a predefined schedule. Predictive Scaling: Forecasting demand and scaling proactively using Machine Learning. Technical Translation\nCompleted the translation of the blog \u0026ldquo;Enhancing Telecommunications Security with AWS\u0026rdquo; from English to Vietnamese. The translation maintains technical accuracy and preserves the original document\u0026rsquo;s structure and formatting. "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I had the opportunity to participate in four professional events. Each event provided meaningful insights, practical knowledge, and valuable experiences that contributed to my academic and professional development.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nMain activities: Engaged in discussions centered on technology transformation in the Industry 4.0 era, with a particular emphasis on the expanding role of artificial intelligence in accelerating digital innovation and organizational productivity.\nLesson learned: Integrating AI-Driven Lifecycle (AI-DLC) methodologies into project execution enhances efficiency and decision-making. Additionally, Generative AI serves as an effective tool for research, knowledge acquisition, and continuous skill development.\nEvent 2 Event Name: AWS Cloud Mastery Series #2 – DevOps on AWS\nDate \u0026amp; Time: 8:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nMain activities: Learned foundational and advanced concepts in DevOps, including CI/CD automation, Infrastructure as Code (IaC) through CloudFormation and the AWS CDK, containerization with ECS, EKS, and App Runner, and observability practices enabled by CloudWatch.\nLesson learned: Modern DevOps practices prioritize collaboration, continuous improvement, and reliable infrastructure orchestration. Applying IaC and AWS container services results in reproducible, scalable, and resilient systems, while enhanced observability supports proactive monitoring and operational excellence.\n"},{"uri":"https://ttjendatgit.github.io/trantiendat/en/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.1-ai-model-integration/","title":"AI-Model-Integration","tags":[],"description":"","content":"Actions to Integrate Hurricane Prediction Model from Lambda Overview Below here we will represent how did we integrate our model from lambda to use step-by-step.\nFiles Currently Using Mock Data 1. WeatherOverlay.tsx (IMPORTANT) Location: frontend/src/components/WeatherOverlay.tsx Mock data: Temperature and Wind overlay data Function: generateWeatherData() Required change: Replace with an API call to Lambda 2. WeeklyForecast.tsx Location: frontend/src/components/WeeklyForecast.tsx Mock data: mockForecast array Required change: Fetch from the backend API 3. windData.ts Location: frontend/src/lib/windData.ts Mock data: mockWindData Required change: Fetch from OpenWeatherMap or Lambda 4. WindFieldManager.ts Location: frontend/src/components/wind/WindFieldManager.ts Mock data: Fallback when there is no API key Status: OK — it already fetches from OpenWeatherMap; you just need to configure the API key How to Integrate the AI Model from Lambda Step 1: Add API endpoints for Storm Prediction In frontend/src/api/weatherApi.ts, add:\nexport interface StormPrediction { stormId: string; name: string; nameVi: string; currentPosition: { lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; }; historicalTrack: Array\u0026lt;{ lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; }\u0026gt;; forecastTrack: Array\u0026lt;{ lat: number; lng: number; timestamp: number; windSpeed: number; pressure: number; category: string; confidence?: number; // Confidence score from the AI model }\u0026gt;; } export const weatherApi = { // ... existing methods ... // Get storm predictions from the Lambda AI model getStormPredictions: async (): Promise\u0026lt;StormPrediction[]\u0026gt; =\u0026gt; { const response = await api.get\u0026lt;StormPrediction[]\u0026gt;(\u0026#39;/storms/predictions\u0026#39;); return response.data; }, // Get details for a specific storm getStormById: async (stormId: string): Promise\u0026lt;StormPrediction\u0026gt; =\u0026gt; { const response = await api.get\u0026lt;StormPrediction\u0026gt;(`/storms/${stormId}`); return response.data; }, }; Step 2: Update the Backend to call Lambda In the C# backend (backend/Controllers/WeatherController.cs), add an endpoint:\n[HttpGet(\u0026#34;storms/predictions\u0026#34;)] public async Task\u0026lt;IActionResult\u0026gt; GetStormPredictions() { try { // Call the Lambda function var lambdaClient = new AmazonLambdaClient(); var request = new InvokeRequest { FunctionName = \u0026#34;storm-prediction-function\u0026#34;, InvocationType = InvocationType.RequestResponse, Payload = \u0026#34;{}\u0026#34; // Or parameters if needed }; var response = await lambdaClient.InvokeAsync(request); using var reader = new StreamReader(response.Payload); var result = await reader.ReadToEndAsync(); return Ok(JsonSerializer.Deserialize\u0026lt;List\u0026lt;StormPrediction\u0026gt;\u0026gt;(result)); } catch (Exception ex) { return StatusCode(500, new { error = ex.Message }); } } Step 3: Update the Frontend to use the real API In frontend/src/pages/Index.tsx (or wherever storm data is fetched):\nimport { weatherApi } from \u0026#39;../api/weatherApi\u0026#39;; import { useQuery } from \u0026#39;@tanstack/react-query\u0026#39;; // Instead of using mock data const { data: storms, isLoading } = useQuery({ queryKey: [\u0026#39;storms\u0026#39;], queryFn: () =\u0026gt; weatherApi.getStormPredictions(), refetchInterval: 5 * 60 * 1000, // Refresh every 5 minutes }); Step 4: Configure Environment Variables Frontend (.env.production):\nVITE_API_BASE_URL=https://your-backend-api.com/api/weather Backend (appsettings.json):\n{ \u0026#34;AWS\u0026#34;: { \u0026#34;Region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;LambdaFunctionName\u0026#34;: \u0026#34;storm-prediction-function\u0026#34; } } Deployment Checklist Deploy the AI model to Lambda Test the Lambda function with sample input Add the API endpoint in the C# backend Test the backend endpoint Update weatherApi.ts with the new endpoints Replace mock data with real API calls Test the frontend with real data Update .env.production with the production URL Build and deploy the frontend Monitor logs and errors Notes Caching: You should cache results from Lambda to reduce cost Error handling: Handle Lambda timeouts or errors gracefully Loading states: Show loading indicators while fetching Fallback: You can keep mock data as a fallback when the API fails Files You Don’t Need to Change (Examples Only) These files are demos/examples and do not affect production:\n*.example.tsx */__tests__/* */GUIDE.md "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.2-fix-unicode-error-solution/","title":"Fix-Unicode-Error","tags":[],"description":"","content":"Fixing UnicodeDecodeError in Lambda There is a series error we want to talk about during the process of development, and this section will talk about it.\nIssue UnicodeDecodeError: \u0026#39;utf-8\u0026#39; codec can\u0026#39;t decode byte 0x80 in position 64: invalid start byte Root Cause The PyTorch model uses the .pth extension (a binary file). The Python runtime also uses .pth files for path configuration (text files). If the model .pth is placed directly in LAMBDA_TASK_ROOT, Python may try to read it as text → causing the error. Applied Fix 1. Update Dockerfile Move the model into a subdirectory models/:\n# Copy model file to subdirectory to avoid Python .pth file confusion RUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ 2. Update app.py Update the model search paths:\npossible_paths = [ \u0026#39;/var/task/models/cropping_storm_7304_2l.pth\u0026#39;, # Primary location in Lambda \u0026#39;models/cropping_storm_7304_2l.pth\u0026#39;, tcn_path ] Rebuild \u0026amp; Deploy Steps Step 1: Build the Docker image cd storm_prediction docker build --provenance=false --platform linux/amd64 -t storm-prediction-model . Note: --provenance=false helps reduce image size for pushing to ECR.\nStep 2: Tag the image docker tag storm-prediction-model:latest 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Step 3: Login to ECR aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com Step 4: Push to ECR docker push 211125445874.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Step 5: Update Lambda AWS Console → Lambda → storm-prediction Open the Image tab Click Deploy new image Select the latest image Click Save Step 6: Test curl -X POST \u0026#34;https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 14.5, \u0026#34;lng\u0026#34;: 121.0}, {\u0026#34;lat\u0026#34;: 14.6, \u0026#34;lng\u0026#34;: 121.1}, {\u0026#34;lat\u0026#34;: 14.7, \u0026#34;lng\u0026#34;: 121.2}, {\u0026#34;lat\u0026#34;: 14.8, \u0026#34;lng\u0026#34;: 121.3}, {\u0026#34;lat\u0026#34;: 14.9, \u0026#34;lng\u0026#34;: 121.4}, {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 121.5}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 121.6}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 121.7}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 121.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; }\u0026#39; Check Logs After Deploy aws logs tail /aws/lambda/storm-prediction --region ap-southeast-1 --follow Summary Before: The .pth model file was in the root → Python mistook it for a config .pth file → UnicodeDecodeError After: The .pth model file is inside models/ → Python ignores it → Lambda works ✅ "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/5-workshop/5.4-frontback-end/5.4.1-frontend-architecture/","title":"Frontend Architecture","tags":[],"description":"","content":"Frontend Architecture - Storm Prediction Web Application Overview Below is the detailed documentation of our front-end development: a React + TypeScript web application for tracking and predicting typhoon trajectories.\nAWS Services Architecture ┌─────────────────────────────────────────────────────────────┐ │ USER BROWSER │ └────────────────────────┬────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────┐ │ CloudFront CDN │ │ - Distribution: d3lj47ilp0fgxy.cloudfront.net │ │ - SSL/TLS: HTTPS │ │ - Cache: Static assets + JSON data │ │ - Origin Access: OAI/OAC (Secure) │ └────────────────────────┬────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────┐ │ S3 Bucket (Private) │ │ - Bucket: storm-frontend-hosting-duc-2025 │ │ - Static Website Hosting: DISABLED │ │ - Access: CloudFront only via REST API │ │ - Content: HTML, CSS, JS, Images, recent_storms.json │ └─────────────────────────────────────────────────────────────┘ ┌─────────────────────────────────────────────────────────────┐ │ Lambda Functions │ │ ┌─────────────────────────────────────────────────────┐ │ │ │ Lambda #1: Storm Prediction │ │ │ │ - URL: vill3povlzqxdyxm7ubldizobu0kdgbi... │ │ │ │ - Method: POST /predict │ │ │ │ - Auth: NONE (public) │ │ │ │ - Container: ECR (Docker) │ │ │ │ - Models: LSTM + TCN │ │ │ └─────────────────────────────────────────────────────┘ │ │ ┌─────────────────────────────────────────────────────┐ │ │ │ Lambda #2: Storm Data Crawler (New) │ │ │ │ - Trigger: EventBridge (Weekly) │ │ │ │ - Function: Crawl IBTrACS data │ │ │ │ - Output: recent_storms.json → S3 │ │ │ └─────────────────────────────────────────────────────┘ │ └─────────────────────────────────────────────────────────────┘ ┌─────────────────────────────────────────────────────────────┐ │ EventBridge │ │ - Rule: storm-data-crawler-weekly-trigger │ │ - Schedule: Every Sunday 00:00 UTC (7AM Vietnam) │ │ - Target: Lambda #2 (Storm Data Crawler) │ └─────────────────────────────────────────────────────────────┘ Frontend Directory Structure frontend/ ├── src/ │ ├── components/ # React components │ │ ├── ui/ # shadcn/ui components (button, card, input, etc.) │ │ ├── storm/ # Storm-specific components │ │ ├── timeline/ # Timeline controls │ │ ├── wind/ # Wind visualization │ │ ├── StormPredictionForm.tsx # Storm coordinate input form │ │ ├── WeatherMap.tsx # Main Leaflet map │ │ ├── StormTracker.tsx # Storm list │ │ ├── StormInfo.tsx # Storm details │ │ ├── StormAnimation.tsx # Animated markers │ │ ├── WeatherOverlay.tsx # Temperature/Wind overlay │ │ ├── WeatherLayerControl.tsx # Satellite/Radar layers │ │ ├── WeatherLayerControlPanel.tsx # Control panel UI │ │ ├── WeatherValueTooltip.tsx # Hover tooltip │ │ ├── WindyLayer.tsx # Windy.com integration │ │ ├── ProvinceLayer.tsx # Vietnam provinces │ │ ├── OptimizedTemperatureLayer.tsx │ │ ├── TemperatureHeatMapLayer.tsx │ │ ├── ThemeToggle.tsx # Dark/Light mode │ │ ├── PreferencesModal.tsx # User preferences │ │ ├── RightSidebar.tsx # Right panel │ │ └── WeeklyForecast.tsx # 7-day forecast │ │ │ ├── pages/ │ │ ├── Index.tsx # Main page │ │ └── NotFound.tsx # 404 page │ │ │ ├── lib/ # Business logic \u0026amp; utilities │ │ ├── api/ # API clients │ │ ├── __tests__/ # Unit tests │ │ ├── stormData.ts # Types \u0026amp; interfaces │ │ ├── stormAnimations.ts # Animation logic │ │ ├── stormIntensityChanges.ts │ │ ├── stormPerformance.ts │ │ ├── stormValidation.ts │ │ ├── windData.ts │ │ ├── windStrengthCalculations.ts │ │ ├── windyStatePersistence.ts │ │ ├── windyUrlState.ts │ │ ├── mapUtils.ts # Map helpers │ │ ├── openWeatherMapClient.ts │ │ ├── dataWorker.ts # Web Worker │ │ ├── utils.ts │ │ └── colorInterpolation.ts │ │ │ ├── hooks/ # Custom React hooks │ │ ├── use-toast.ts │ │ ├── use-theme.tsx │ │ ├── use-mobile.tsx │ │ ├── useTimelineState.ts │ │ ├── useWindyStateSync.ts │ │ └── useSimplifiedTooltip.ts │ │ │ ├── contexts/ # React Context │ │ └── WindyStateContext.tsx │ │ │ ├── api/ │ │ └── weatherApi.ts # API calls │ │ │ ├── utils/ │ │ └── colorInterpolation.ts │ │ │ ├── styles/ │ │ └── accessibility.css # WCAG compliance styles │ │ │ ├── test/ # Test suite │ │ ├── accessibility.test.ts │ │ ├── accessibility-audit.test.ts │ │ ├── wcag-compliance.test.ts │ │ ├── performance.test.ts │ │ ├── cross-browser.test.ts │ │ └── setup.ts │ │ │ ├── assets/ # Images, icons │ ├── App.tsx │ ├── main.tsx │ └── index.css │ ├── public/ # Static assets ├── dist/ # Build output (after npm run build) ├── .env.production # Production config ├── .env.example ├── package.json ├── vite.config.ts ├── vitest.config.ts # Test config ├── tailwind.config.ts ├── tsconfig.json └── components.json # shadcn/ui config Environment Variables .env.production # OpenWeather API VITE_OPENWEATHER_API_KEY=8ff7f009d2bd420c86845c6bcf6de4a9 # CloudFront URL - Fetch storm data VITE_CLOUDFRONT_URL=https://d3lj47ilp0fgxy.cloudfront.net # Lambda Function URL - Storm prediction API VITE_PREDICTION_API_URL=https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws ** Screenshots needed:**\nAWS CloudFront → Distributions → Distribution domain name AWS Lambda → storm-prediction → Function URL Build \u0026amp; Deploy Process 1. Build Production cd frontend npm run build Output: dist/ folder contains:\nindex.html assets/index-[hash].js assets/index-[hash].css 2. Upload to S3 aws s3 sync dist/ s3://storm-frontend-hosting-duc-2025/ --delete Important Notes:\nS3 bucket is PRIVATE (no public access) CloudFront uses REST API endpoint, not website endpoint Origin: storm-frontend-hosting-duc-2025.s3.ap-southeast-1.amazonaws.com\n3. Invalidate CloudFront Cache aws cloudfront create-invalidation \\ --distribution-id E1234567890ABC \\ --paths \u0026#34;/*\u0026#34; Data Flow A. Load Storm Data (Startup) Browser → CloudFront → S3 ↓ GET /recent_storms.json ↓ Parse JSON → Display on map File: src/pages/Index.tsx (line ~40)\nconst CLOUDFRONT_URL = import.meta.env.VITE_CLOUDFRONT_URL; const FETCH_URL = `${CLOUDFRONT_URL}/recent_storms.json?t=${Date.now()}`; B. Storm Prediction (User Action) User fills form → Click \u0026#34;Run Prediction\u0026#34; ↓ POST /predict to Lambda Function URL ↓ { \u0026#34;history\u0026#34;: [{lat, lng}, ...], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } ↓ Lambda processes → Returns forecast ↓ Display predicted path on map File: src/components/StormPredictionForm.tsx (line ~80)\nconst API_URL = `${import.meta.env.VITE_PREDICTION_API_URL}/predict`; const response = await fetch(API_URL, { method: \u0026#34;POST\u0026#34;, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }, body: JSON.stringify({ history, storm_name }) }); Key Components 1. Core Components StormPredictionForm File: src/components/StormPredictionForm.tsx\nFeatures:\nForm for inputting storm coordinates (min 9 points) Input validation (valid lat/lng) Calls Lambda API for prediction Displays results on map Scrollable list with Add/Remove positions Props:\ninterface StormPredictionFormProps { onPredictionResult: (result: PredictionResult) =\u0026gt; void; setIsLoading: (isLoading: boolean) =\u0026gt; void; } WeatherMap File: src/components/WeatherMap.tsx\nFeatures:\nDisplays Leaflet map Renders storm tracks (historical + forecast) Renders prediction path (purple, dashed) Weather overlays (temperature, wind, radar) Multiple storm rendering Auto-zoom to selected storm Custom panes for z-index layering Props:\ninterface WeatherMapProps { storms: Storm[]; selectedStorm?: Storm; customPrediction?: PredictionResult | null; mapFocusBounds?: LatLngBounds | null; onMapFocusComplete?: () =\u0026gt; void; } Index (Main Page) File: src/pages/Index.tsx\nFeatures:\nMain layout with header/footer State management (storms, selectedStorm, customPrediction) Sidebar with tabs (Current Storms / Predict Storm) Timeline state synchronization Loading \u0026amp; error handling Skip links for accessibility 2. Storm Components StormTracker File: src/components/StormTracker.tsx\nList of current storms Filter by status (active/developing/dissipated) Click to select storm StormInfo File: src/components/StormInfo.tsx\nDetailed storm information Wind speed, pressure, category Historical data Forecast timeline StormAnimation File: src/components/StormAnimation.tsx\nAnimated markers for storm positions Pulsing effect Category-based colors 3. Weather Layer Components WeatherOverlay File: src/components/WeatherOverlay.tsx\nTemperature heatmap overlay Wind speed visualization Real-time data from OpenWeather API Hover to view values WeatherLayerControl File: src/components/WeatherLayerControl.tsx\nSatellite imagery layer Radar layer Temperature layer Tile layer management WeatherLayerControlPanel File: src/components/WeatherLayerControlPanel.tsx\nUI controls for weather layers Opacity slider Layer toggle buttons Temperature animation toggle OptimizedTemperatureLayer \u0026amp; TemperatureHeatMapLayer Files: src/components/OptimizedTemperatureLayer.tsx, TemperatureHeatMapLayer.tsx\nPerformance-optimized temperature rendering Color interpolation Grid-based heatmap 4. Wind Components WindyLayer File: src/components/WindyLayer.tsx\nWindy.com iframe integration Wind animation overlay Synchronized state with main map Context: src/contexts/WindyStateContext.tsx\nGlobal state for Windy layer URL state persistence Sync across components 5. Map Enhancement Components ProvinceLayer File: src/components/ProvinceLayer.tsx\nVietnam provinces boundaries GeoJSON rendering Province labels WeatherValueTooltip File: src/components/WeatherValueTooltip.tsx\nTooltip displaying weather values on hover Temperature, wind speed, pressure Positioned tooltip 6. UI Components ThemeToggle File: src/components/ThemeToggle.tsx\nDark/Light mode switch Persisted preference System theme detection PreferencesModal File: src/components/PreferencesModal.tsx\nUser preferences settings Map options Display preferences RightSidebar File: src/components/RightSidebar.tsx\nAdditional info panel Collapsible sidebar WeeklyForecast File: src/components/WeeklyForecast.tsx\n7-day weather forecast Temperature trends Weather icons 7. Timeline Components Folder: src/components/timeline/\nTimeline controls for storm animation Play/Pause functionality Time scrubbing Speed controls Data Types PredictionResult File: src/lib/stormData.ts\nexport interface PredictionResult { storm_id: string; storm_name: string; prediction_time: string; totalDistance: number; // km actualDistance: number; // km lifespan: number; // hours forecastHours: number; // hours forecast: StormPoint[]; // Predicted positions path?: StormPoint[]; // Legacy support } StormPoint export interface StormPoint { timestamp: number; // Unix timestamp (ms) lat: number; lng: number; windSpeed: number; // km/h pressure: number; // hPa category: string; // \u0026#34;Typhoon\u0026#34;, \u0026#34;Super Typhoon\u0026#34;, etc. } AWS Permissions Required S3 Bucket Policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::storm-frontend-hosting-duc-2025/*\u0026#34; } ] } CloudFront Origin Access Origin: S3 bucket Origin Access: Public (or OAI if used) Testing Local Development npm run dev # Open http://localhost:5173 Production Build Test npm run build npm run preview # Open http://localhost:4173 📸 Screenshots needed:\nBrowser DevTools → Network tab → API calls Browser DevTools → Console → No errors Common Issues 1. CORS Error when calling Lambda Symptom: Access-Control-Allow-Origin error\nSolution: Lambda must return CORS headers:\nreturn { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } 2. CloudFront stale cache Symptom: New code not showing\nSolution: Invalidate cache\naws cloudfront create-invalidation --distribution-id E... --paths \u0026#34;/*\u0026#34; 3. Environment variables not loading Symptom: undefined when accessing import.meta.env.VITE_*\nSolution:\nEnsure .env.production file exists Rebuild: npm run build Variables must start with VITE_ Deployment Checklist Update .env.production with correct URLs npm run build succeeds Upload dist/ to S3 Invalidate CloudFront cache Test on production URL Verify Lambda API works Verify storm data loads Test prediction form with 9+ positions API Endpoints 1. Get Storm Data GET https://d3lj47ilp0fgxy.cloudfront.net/recent_storms.json Response: Array of Storm objects\n2. Predict Storm Path POST https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict Body: { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, ... ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } Response: { \u0026#34;storm_id\u0026#34;: \u0026#34;unknown\u0026#34;, \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34;, \u0026#34;totalDistance\u0026#34;: 500.5, \u0026#34;lifespan\u0026#34;: 72, \u0026#34;forecast\u0026#34;: [...] } Performance Optimization 1. Code Optimization Code Splitting: Vite automatically splits chunks by routes Tree Shaking: Remove unused code Minification: Production build auto-minifies JS/CSS Lazy Loading: Components load on demand 2. Data Optimization Web Workers: Heavy computations run in worker (dataWorker.ts) Memoization: React.memo for expensive components Debouncing: Input handlers are debounced Caching: LocalStorage cache for preferences 3. Rendering Optimization Virtual Scrolling: Large lists use virtual scrolling Optimized Layers: OptimizedTemperatureLayer for performance Canvas Rendering: Heatmap uses canvas instead of DOM Pane Management: Custom Leaflet panes for z-index optimization 4. Network Optimization CDN Caching: CloudFront caches static assets Image Optimization: WebP format, lazy loading API Caching: Cache storm data with timestamp Compression: Gzip/Brotli compression 5. Accessibility Performance Skip Links: Keyboard navigation shortcuts ARIA Labels: Proper semantic HTML Focus Management: Logical tab order Screen Reader: Optimized for screen readers Libraries \u0026amp; Utilities Business Logic (lib/) Storm Management stormData.ts: Types, interfaces, Storm/StormPoint definitions stormAnimations.ts: Animation logic for storm markers stormIntensityChanges.ts: Storm intensity change calculations stormPerformance.ts: Performance optimization for rendering stormValidation.ts: Storm data validation Wind System windData.ts: Wind data structures windStrengthCalculations.ts: Wind strength calculations windyStatePersistence.ts: Windy layer state persistence windyUrlState.ts: URL state management for Windy Map \u0026amp; Weather mapUtils.ts: Map helpers (center, zoom, bounds calculations) openWeatherMapClient.ts: OpenWeather API client colorInterpolation.ts: Color gradient calculations Performance dataWorker.ts: Web Worker for heavy computations utils.ts: General utilities Custom Hooks (hooks/) use-toast.ts: Toast notification system use-theme.tsx: Dark/Light theme management use-mobile.tsx: Mobile device detection useTimelineState.ts: Timeline state synchronization useWindyStateSync.ts: Windy layer state sync useSimplifiedTooltip.ts: Simplified tooltip logic Context (contexts/) WindyStateContext.tsx: Global state for Windy layer integration Testing (test/) accessibility.test.ts: Accessibility testing accessibility-audit.test.ts: WCAG audit wcag-compliance.test.ts: WCAG 2.1 compliance performance.test.ts: Performance benchmarks cross-browser.test.ts: Cross-browser compatibility setup.ts: Test environment setup Dependencies Core React 18 TypeScript Vite (build tool) Vitest (testing) UI Framework Tailwind CSS shadcn/ui (component library) Lucide Icons Radix UI (primitives) Map \u0026amp; Visualization Leaflet React-Leaflet GeoJSON support API \u0026amp; Data Fetch API (native) OpenWeather API AWS Lambda Function URL State Management React Context API URL state (query params) LocalStorage persistence Performance Web Workers Code splitting (Vite) Lazy loading Screenshots Cloudfront Distribution Figure 1 Origin Settings Figure 1 Invalidations Figure 2 storm-frontend-hosting-duc-2025 Figure 3 Permissions Figure 4 storm-ai-models-2025 Figure 5 storm-data-store-2025 Figure 6 Main Page Figure 7 Storm Tracking Features Figure 8 Storm Details Figure 9 Predict Feature Figure 10 Figure 11 Figure 12 "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/","title":"Lambda Architecture","tags":[],"description":"","content":"Lambda Architecture - Storm Prediction AI Service Overview Lambda functions are an important component of a serverless architecture. They are especially useful due to their cost-effectiveness and ease of deployment—both of which are valuable for our hurricane prediction platform.\nThis section presents the details of how we designed and built our Lambda architecture.\nOur Lambda functions run PyTorch models for typhoon trajectory prediction and are deployed using a Docker container image.\nAWS Services Architecture ┌─────────────────────────────────────────────────────────────┐ │ Frontend (Browser) │ └────────────────────────┬────────────────────────────────────┘ │ POST /predict ▼ ┌─────────────────────────────────────────────────────────────┐ │ Lambda Function URL (Public) │ │ URL: https://vill3povlzqxdyxm7ubldizobu0kdgbi... │ │ Auth: NONE │ │ Method: POST │ └────────────────────────┬────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────┐ │ Lambda Function │ │ Name: storm-prediction │ │ Runtime: Python 3.10 (Container) │ │ Memory: 3008 MB │ │ Timeout: 120 seconds │ │ Architecture: x86_64 │ └────────────────────────┬────────────────────────────────────┘ │ ▼ ┌─────────────────────────────────────────────────────────────┐ │ ECR Repository │ │ Account: 339570693867 │ │ Region: ap-southeast-1 │ │ Repo: storm-prediction │ │ Image: latest │ │ Size: ~2 GB │ └─────────────────────────────────────────────────────────────┘ ┌─────────────────────────────────────────────────────────────┐ │ S3 Buckets │ │ 1. storm-frontend-hosting-duc-2025 │ │ - models/lstm_totald_256_4.pt (optional) │ │ - predictions/[storm_id]_[timestamp].json │ │ │ │ 2. storm-ai-models (recommended) │ │ - models/lstm_totald_256_4.pt │ │ - models/tcn_model.pth (backup) │ └─────────────────────────────────────────────────────────────┘ storm_prediction/ Directory Structure storm_prediction/ ├── app.py # Lambda handler (main code) ├── Dockerfile # Container definition ├── requirements.txt # Python dependencies ├── cropping_storm_7304_2l.pth # TCN model (included in image) │ ├── DEPLOY_NOW.md # Quick deploy guide ├── DEPLOY_CONSOLE_STEP_BY_STEP.md # AWS Console guide ├── LAMBDA_DEPLOYMENT_GUIDE.md # Detailed deployment ├── AWS_CONSOLE_DEPLOYMENT_GUIDE.md ├── FIX_ECR_PUSH_ERROR.md # Troubleshooting ├── FIX_UNICODE_ERROR.md # UnicodeDecodeError fix ├── FIX_UNICODE_ERROR_SOLUTION.md # Solution details └── REBUILD_AND_DEPLOY.sh # Automated script Docker Image Structure Dockerfile FROM public.ecr.aws/lambda/python:3.10 # Install dependencies COPY requirements.txt . RUN pip3 install -r requirements.txt \\ --target \u0026#34;${LAMBDA_TASK_ROOT}\u0026#34; \\ --extra-index-url https://download.pytorch.org/whl/cpu # Copy Lambda handler COPY app.py ${LAMBDA_TASK_ROOT} # Copy TCN model to subdirectory (avoid .pth confusion) RUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ # Set handler CMD [ \u0026#34;app.handler\u0026#34; ] Image Layers Layer 1: AWS Lambda Python 3.10 base (~500 MB) Layer 2: PyTorch CPU + dependencies (~1.2 GB) Layer 3: app.py + TCN model (~300 MB) ───────────────────────────────────────────── Total: ~2 GB AI Models 1. TCN Model (Trajectory Prediction) File: cropping_storm_7304_2l.pth Location: Inside Docker image at /var/task/models/ Size: ~300 MB Purpose: Predict next step (lat, lng) of typhoon trajectory\nArchitecture:\nclass StormTCN(nn.Module): def __init__(self, input_dim=4, hidden_units=1024, num_layers=2): self.tcn = TCN(...) self.head_latlon = nn.Linear(hidden_units, 2) # Predict lat, lng self.head_aux = nn.Linear(hidden_units, 2) # Predict aux features Input: [batch, sequence, 4] - (lat, lng, distance, bearing) Output:\npred_latlon: Next (lat, lng) pred_aux: Auxiliary features 2. LSTM Model (Total Distance Prediction) File: lstm_totald_256_4.pt Location: S3 bucket (downloaded on first use) Size: ~50 MB Purpose: Predict total distance typhoon will travel\nArchitecture:\nclass StormLSTM(nn.Module): def __init__(self, input_size=4, hidden_size=256, num_layers=2): self.lstm = nn.LSTM(...) self.fc = nn.Sequential( nn.Linear(hidden_size, hidden_size // 2), nn.ReLU(), nn.Linear(hidden_size // 2, 1) # Predict total distance ) Input: Daily summary [batch, days, 4] - (day, daily_dist, avg_speed, motion_type) Output: Total distance (km)\nRequest Flow 1. Receive Request POST /predict Content-Type: application/json { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, ... // Min 9 points ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34;, \u0026#34;storm_id\u0026#34;: \u0026#34;TEST001\u0026#34; // Optional } 2. Load Models (First Invocation Only) def load_models(): global LSTM_MODEL, TCN_MODEL # Load LSTM from S3 (if available) if not os.path.exists(\u0026#39;/tmp/lstm_model.pt\u0026#39;): s3_client.download_file( MODEL_BUCKET, \u0026#39;models/lstm_totald_256_4.pt\u0026#39;, \u0026#39;/tmp/lstm_model.pt\u0026#39; ) LSTM_MODEL = StormLSTM(...) LSTM_MODEL.load_state_dict(torch.load(\u0026#39;/tmp/lstm_model.pt\u0026#39;)) # Load TCN from local (already in image) TCN_MODEL = StormTCN(...) TCN_MODEL.load_state_dict( torch.load(\u0026#39;/var/task/models/cropping_storm_7304_2l.pth\u0026#39;) ) 3. Preprocess Input def preprocess_history(history): # Convert to tensor [1, sequence_length, 4] # Features: [lat, lng, distance, bearing] processed = [] for i in range(len(history)): if i == 0: processed.append([lat, lng, 0.0, 0.0]) else: dist = haversine(prev_lat, prev_lng, lat, lng) brng = bearing(prev_lat, prev_lng, lat, lng) processed.append([lat, lng, dist, brng]) return torch.tensor(processed).unsqueeze(0) 4. Predict Total Distance (LSTM) def predict_total_distance(record_tensor): if LSTM_MODEL is None: # Fallback: avg_distance * 24 steps return fallback_distance # Group by day (9 points/day) # Run LSTM prediction with torch.no_grad(): pred = LSTM_MODEL(summary_tensor, lengths) return pred.item() # km 5. Predict Path (TCN) def predict_storm_path(record_tensor, total_distance, history): seq = record_tensor.clone() gone_distance = 0 predicted_points = [] while gone_distance \u0026lt; total_distance: # Predict next position pred_latlon, pred_aux = TCN_MODEL(seq) new_lat = pred_latlon[0, -1, 0].item() new_lng = pred_latlon[0, -1, 1].item() # Calculate distance \u0026amp; bearing step_distance = haversine(last_lat, last_lng, new_lat, new_lng) # Estimate windspeed (decay over time) estimated_wind = max(avg_wind * (0.98 ** step), 30) predicted_points.append({ \u0026#39;lat\u0026#39;: new_lat, \u0026#39;lng\u0026#39;: new_lng, \u0026#39;timestamp\u0026#39;: base_timestamp + (step * 3 * 3600 * 1000), \u0026#39;windSpeed\u0026#39;: estimated_wind, \u0026#39;pressure\u0026#39;: 980.0, \u0026#39;category\u0026#39;: calculate_category(estimated_wind) }) # Update sequence (sliding window) seq = torch.cat([seq[:, 1:, :], next_point.unsqueeze(1)], dim=1) gone_distance += step_distance step += 1 return predicted_points 6. Return Response result = { \u0026#39;storm_id\u0026#39;: storm_id, \u0026#39;storm_name\u0026#39;: storm_name, \u0026#39;prediction_time\u0026#39;: datetime.now().isoformat(), \u0026#39;totalDistance\u0026#39;: 500.5, \u0026#39;actualDistance\u0026#39;: 520.3, \u0026#39;lifespan\u0026#39;: 72, \u0026#39;forecastHours\u0026#39;: 72, \u0026#39;forecast\u0026#39;: [ { \u0026#39;lat\u0026#39;: 15.1, \u0026#39;lng\u0026#39;: 106.99, \u0026#39;timestamp\u0026#39;: 1765015351626, \u0026#39;windSpeed\u0026#39;: 65, \u0026#39;pressure\u0026#39;: 980, \u0026#39;category\u0026#39;: \u0026#39;Typhoon\u0026#39; }, ... ] } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } Build \u0026amp; Deploy Process Step 1: Build Docker Image cd storm_prediction docker build \\ --provenance=false \\ --platform linux/amd64 \\ -t storm-prediction-model . Flags:\n--provenance=false: Reduce image size (no build metadata) --platform linux/amd64: Lambda only supports x86_64 -t storm-prediction-model: Tag name Step 2: Tag for ECR docker tag storm-prediction-model:latest \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Step 3: Login to ECR aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com Step 4: Push to ECR docker push \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction:latest Time: ~5-10 minutes (2GB upload)\nStep 5: Update Lambda Function AWS Console:\nLambda → storm-prediction Tab Image → Deploy new image Select latest image Click Save Lambda Configuration Function Settings Name: storm-prediction Runtime: Container image Architecture: x86_64 Memory: 3008 MB Timeout: 120 seconds Ephemeral storage: 512 MB Environment Variables MODEL_BUCKET=storm-frontend-hosting-duc-2025 DATA_BUCKET=storm-frontend-hosting-duc-2025 Function URL URL: https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws Auth type: NONE CORS: Enabled - Allow origins: * - Allow methods: POST, OPTIONS - Allow headers: Content-Type IAM Role Permissions { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::storm-frontend-hosting-duc-2025/*\u0026#34;, \u0026#34;arn:aws:s3:::storm-ai-models/*\u0026#34; ] } ] } Monitoring \u0026amp; Logs CloudWatch Logs Log Group: /aws/lambda/storm-prediction\nKey Log Messages:\nLoading LSTM model... Downloaded LSTM from S3 LSTM loaded successfully Loading TCN model... Checking: /var/task/models/cropping_storm_7304_2l.pth Found TCN at /var/task/models/cropping_storm_7304_2l.pth TCN loaded successfully Processing: Test Storm (TEST001) Input points: 9 Predicted total distance: 500.50 km Generated 24 predictions (72 hours) Saved to S3: predictions/TEST001_1733486400.json Screenshots needed:\nCloudWatch → Log groups → /aws/lambda/storm-prediction Log stream → Recent logs with emojis Logs → Duration, Memory used Metrics CloudWatch Metrics:\nInvocations Duration (avg ~5-10 seconds) Errors Throttles Memory used (~500-800 MB) Screenshots needed:\nLambda → Monitor → Metrics CloudWatch → Metrics → Lambda → Function metrics Common Issues \u0026amp; Solutions 1. UnicodeDecodeError: \u0026lsquo;utf-8\u0026rsquo; codec can\u0026rsquo;t decode byte 0x80 Symptom:\nUnicodeDecodeError: \u0026#39;utf-8\u0026#39; codec can\u0026#39;t decode byte 0x80 in position 64 Cause: Model .pth file at root mistaken as Python config file\nSolution: Move model to subdirectory\nRUN mkdir -p ${LAMBDA_TASK_ROOT}/models COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/models/ 2. 502 Bad Gateway Symptom: Frontend receives 502 error\nCauses:\nLambda timeout (exceeds 120s) Lambda crash (out of memory) Model failed to load Solutions:\nCheck CloudWatch Logs Increase memory if needed Increase timeout if needed 3. LSTM Fallback Symptom: Log shows \u0026ldquo;⚠️ Using fallback distance\u0026rdquo;\nCause: LSTM model not on S3\nSolution: Upload lstm_totald_256_4.pt to S3:\naws s3 cp lstm_totald_256_4.pt \\ s3://storm-frontend-hosting-duc-2025/models/ 4. ECR Push 403 Forbidden Symptom: 403 Forbidden when pushing image\nCauses:\nECR login expired Wrong account ID Repository doesn\u0026rsquo;t exist Solutions:\n# Re-login aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ 339570693867.dkr.ecr.ap-southeast-1.amazonaws.com # Create repository if needed aws ecr create-repository \\ --repository-name storm-prediction \\ --region ap-southeast-1 Testing Local Test (if possible) # Run locally python app.py # Test event test_event = { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, ... ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } result = handler(test_event, None) print(result) Lambda Test AWS Console:\nLambda → Test tab Create test event: { \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 120.2}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 120.3}, {\u0026#34;lat\u0026#34;: 15.4, \u0026#34;lng\u0026#34;: 120.4}, {\u0026#34;lat\u0026#34;: 15.5, \u0026#34;lng\u0026#34;: 120.5}, {\u0026#34;lat\u0026#34;: 15.6, \u0026#34;lng\u0026#34;: 120.6}, {\u0026#34;lat\u0026#34;: 15.7, \u0026#34;lng\u0026#34;: 120.7}, {\u0026#34;lat\u0026#34;: 15.8, \u0026#34;lng\u0026#34;: 120.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; } Click Test Check response cURL Test curl -X POST \\ \u0026#34;https://vill3povlzqxdyxm7ubldizobu0kdgbi.lambda-url.ap-southeast-1.on.aws/predict\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;history\u0026#34;: [ {\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 120.0}, {\u0026#34;lat\u0026#34;: 15.1, \u0026#34;lng\u0026#34;: 120.1}, {\u0026#34;lat\u0026#34;: 15.2, \u0026#34;lng\u0026#34;: 120.2}, {\u0026#34;lat\u0026#34;: 15.3, \u0026#34;lng\u0026#34;: 120.3}, {\u0026#34;lat\u0026#34;: 15.4, \u0026#34;lng\u0026#34;: 120.4}, {\u0026#34;lat\u0026#34;: 15.5, \u0026#34;lng\u0026#34;: 120.5}, {\u0026#34;lat\u0026#34;: 15.6, \u0026#34;lng\u0026#34;: 120.6}, {\u0026#34;lat\u0026#34;: 15.7, \u0026#34;lng\u0026#34;: 120.7}, {\u0026#34;lat\u0026#34;: 15.8, \u0026#34;lng\u0026#34;: 120.8} ], \u0026#34;storm_name\u0026#34;: \u0026#34;Test Storm\u0026#34; }\u0026#39; Deployment Checklist Model file cropping_storm_7304_2l.pth exists (Optional) Upload LSTM model to S3 Build Docker image successfully Tag image with correct account ID (339570693867) Login to ECR successfully Push image to ECR Update Lambda function with new image Check Lambda configuration (memory, timeout) Test Lambda with test event Test via Function URL with cURL Test from frontend Check CloudWatch Logs Verify prediction results on map Screenshots Function Figure 1 Configuration Figure 2 Environment Variables Figure 3 ECR Repository Figure 4 Figure 5 "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/5-workshop/5.4-frontback-end/5.4.2-lambda-architecture/5.4.2.3-lambda-deployment/","title":"Lamda-Deployment","tags":[],"description":"","content":"Step to deploy our PyTorch Model on storm prediction on AWS Lambda AWS Lambda deployment plays a critical role in our website development pipeline. In this section, we document the process we followed to successfully complete the deployment.\nStep 1: Prepare the Code 1.1. Update app.py import json import torch import numpy as np from typing import List, Dict MODEL_PATH = \u0026#34;model.pth\u0026#34; device = torch.device(\u0026#34;cpu\u0026#34;) model = None def load_model(): global model if model is None: print(f\u0026#34;Loading model from {MODEL_PATH}...\u0026#34;) model = torch.load(MODEL_PATH, map_location=device) model.eval() print(\u0026#34;Model loaded successfully!\u0026#34;) return model def prepare_features(history: List[Dict]) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34; Convert history into a tensor for the model history: [{\u0026#34;lat\u0026#34;: 15.0, \u0026#34;lng\u0026#34;: 107.0}, ...] \u0026#34;\u0026#34;\u0026#34; # TODO: Implement feature engineering based on your model lats = [p[\u0026#34;lat\u0026#34;] for p in history] lngs = [p[\u0026#34;lng\u0026#34;] for p in history] features = np.array([lats + lngs]) # Shape: (1, 18) return torch.tensor(features, dtype=torch.float32) def format_predictions(predictions: torch.Tensor, storm_name: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34; Format output to match what the frontend expects \u0026#34;\u0026#34;\u0026#34; pred_array = predictions.detach().cpu().numpy()[0] forecast = [] base_timestamp = int(time.time() * 1000) for i in range(0, len(pred_array), 2): if i + 1 \u0026lt; len(pred_array): forecast.append({ \u0026#34;lat\u0026#34;: float(pred_array[i]), \u0026#34;lng\u0026#34;: float(pred_array[i + 1]), \u0026#34;timestamp\u0026#34;: base_timestamp + (i // 2) * 3600000, # +1 hour each \u0026#34;windSpeed\u0026#34;: 120.0, # TODO: Predict from model \u0026#34;pressure\u0026#34;: 980.0, # TODO: Predict from model \u0026#34;category\u0026#34;: \u0026#34;Category 3\u0026#34;, # TODO: Classify from windSpeed \u0026#34;confidence\u0026#34;: 0.85 }) return { \u0026#34;storm_name\u0026#34;: storm_name, \u0026#34;forecast\u0026#34;: forecast } def handler(event, context): \u0026#34;\u0026#34;\u0026#34; Lambda handler function \u0026#34;\u0026#34;\u0026#34; try: # Parse input body = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) history = body.get(\u0026#39;history\u0026#39;, []) storm_name = body.get(\u0026#39;storm_name\u0026#39;, \u0026#39;Unknown Storm\u0026#39;) # Validate input if len(history) \u0026lt; 9: return { \u0026#39;statusCode\u0026#39;: 400, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: f\u0026#39;Need at least 9 positions, got {len(history)}\u0026#39; }) } # Load model model = load_model() # Prepare features X = prepare_features(history) # Predict with torch.no_grad(): predictions = model(X) # Format output result = format_predictions(predictions, storm_name) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(result) } except Exception as e: print(f\u0026#34;Error: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: str(e) }) } 1.2. Update Dockerfile FROM public.ecr.aws/lambda/python:3.11 # Copy requirements and install COPY requirements.txt ${LAMBDA_TASK_ROOT} RUN pip install --no-cache-dir -r requirements.txt # Copy model (rename to model.pth) COPY cropping_storm_7304_2l.pth ${LAMBDA_TASK_ROOT}/model.pth # Copy code COPY app.py ${LAMBDA_TASK_ROOT} # Set handler CMD [\u0026#34;app.handler\u0026#34;] 1.3. Verify requirements.txt torch==2.1.0 numpy==1.24.3 Step 2: Build the Docker Image cd storm_prediction # Build image docker build -t storm-prediction-model . # Test locally (optional) docker run -p 9000:8080 storm-prediction-model # Test with curl curl -X POST \u0026#34;http://localhost:9000/2015-03-31/functions/function/invocations\u0026#34; \\ -d \u0026#39;{ \u0026#34;body\u0026#34;: \u0026#34;{\\\u0026#34;history\\\u0026#34;: [{\\\u0026#34;lat\\\u0026#34;: 15.0, \\\u0026#34;lng\\\u0026#34;: 107.0}, {\\\u0026#34;lat\\\u0026#34;: 15.1, \\\u0026#34;lng\\\u0026#34;: 107.1}, {\\\u0026#34;lat\\\u0026#34;: 15.2, \\\u0026#34;lng\\\u0026#34;: 107.2}, {\\\u0026#34;lat\\\u0026#34;: 15.3, \\\u0026#34;lng\\\u0026#34;: 107.3}, {\\\u0026#34;lat\\\u0026#34;: 15.4, \\\u0026#34;lng\\\u0026#34;: 107.4}, {\\\u0026#34;lat\\\u0026#34;: 15.5, \\\u0026#34;lng\\\u0026#34;: 107.5}, {\\\u0026#34;lat\\\u0026#34;: 15.6, \\\u0026#34;lng\\\u0026#34;: 107.6}, {\\\u0026#34;lat\\\u0026#34;: 15.7, \\\u0026#34;lng\\\u0026#34;: 107.7}, {\\\u0026#34;lat\\\u0026#34;: 15.8, \\\u0026#34;lng\\\u0026#34;: 107.8}], \\\u0026#34;storm_name\\\u0026#34;: \\\u0026#34;Test Storm\\\u0026#34;}\u0026#34; }\u0026#39; Step 3: Upload to AWS ECR # 1. Create ECR repository aws ecr create-repository \\ --repository-name storm-prediction-model \\ --region ap-southeast-1 # 2. Login Docker to ECR aws ecr get-login-password --region ap-southeast-1 | \\ docker login --username AWS --password-stdin \\ \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com # 3. Tag image docker tag storm-prediction-model:latest \\ \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest # 4. Push image docker push \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest Step 4: Create the Lambda Function 4.1. Create Lambda from Console Open AWS Lambda Console Click “Create function” Choose “Container image” Function name: storm-prediction Container image URI: select the image you pushed to ECR Architecture: x86_64 Click “Create function” 4.2. Configure Lambda # Or use AWS CLI aws lambda create-function \\ --function-name storm-prediction \\ --package-type Image \\ --code ImageUri=\u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/storm-prediction-model:latest \\ --role arn:aws:iam::\u0026lt;account-id\u0026gt;:role/lambda-execution-role \\ --timeout 60 \\ --memory-size 3008 \\ --region ap-southeast-1 Important configuration:\nMemory: 3008 MB (PyTorch models need RAM) Timeout: 60 seconds (inference can take 10–30s) Ephemeral storage: 512 MB (default; increase if needed) 4.3. Create IAM Role Lambda needs a role with permissions:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:GetDownloadUrlForLayer\u0026#34;, \u0026#34;ecr:BatchGetImage\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Step 5: Create API Gateway # 1. Create REST API aws apigateway create-rest-api \\ --name storm-prediction-api \\ --region ap-southeast-1 # 2. Get API ID and Root Resource ID API_ID=\u0026lt;your-api-id\u0026gt; ROOT_ID=\u0026lt;your-root-resource-id\u0026gt; # 3. Create resource /predict aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part predict # 4. Create POST method RESOURCE_ID=\u0026lt;predict-resource-id\u0026gt; aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $RESOURCE_ID \\ --http-method POST \\ --authorization-type NONE # 5. Integrate with Lambda aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $RESOURCE_ID \\ --http-method POST \\ --type AWS_PROXY \\ --integration-http-method POST \\ --uri arn:aws:apigateway:ap-southeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-southeast-1:\u0026lt;account-id\u0026gt;:function:storm-prediction/invocations # 6. Deploy API aws apigateway create-deployment \\ --rest-api-id $API_ID \\ --stage-name prod API URL: https://\u0026lt;api-id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/prod/predict\nStep 6: Update the Frontend 6.1. Update .env.production VITE_PREDICTION_API_URL=https://\u0026lt;api-id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/prod 6.2. Build \u0026amp; deploy frontend cd frontend npm run build # Deploy dist/ to S3/CloudFront Optimizations 1. Reduce Cold Start Provisioned Concurrency:\naws lambda put-provisioned-concurrency-config \\ --function-name storm-prediction \\ --provisioned-concurrent-executions 1 \\ --qualifier $LATEST 2. Reduce Image Size Use PyTorch CPU-only:\n# requirements.txt torch==2.1.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu numpy==1.24.3 Multi-stage build:\n# Stage 1: Build FROM python:3.11-slim as builder COPY requirements.txt . RUN pip install --target /packages -r requirements.txt # Stage 2: Runtime FROM public.ecr.aws/lambda/python:3.11 COPY --from=builder /packages ${LAMBDA_RUNTIME_DIR} COPY model.pth ${LAMBDA_TASK_ROOT}/ COPY app.py ${LAMBDA_TASK_ROOT}/ CMD [\u0026#34;app.handler\u0026#34;] 3. Cache the Model in /tmp import os MODEL_PATH = \u0026#34;/tmp/model.pth\u0026#34; if os.path.exists(\u0026#34;/tmp/model.pth\u0026#34;) else \u0026#34;model.pth\u0026#34; def load_model(): global model if model is None: # Copy to /tmp for faster access if not os.path.exists(\u0026#34;/tmp/model.pth\u0026#34;): import shutil shutil.copy(\u0026#34;model.pth\u0026#34;, \u0026#34;/tmp/model.pth\u0026#34;) model = torch.load(\u0026#34;/tmp/model.pth\u0026#34;, map_location=device) model.eval() return model Monitoring CloudWatch Logs `bash\nView logs aws logs tail /aws/lambda/storm-prediction \u0026ndash;follow `\nCloudWatch Metrics Invocations: number of calls Duration: runtime Errors: error count Throttles: throttled invocations Alerts # Create an alarm for errors aws cloudwatch put-metric-alarm \\ --alarm-name storm-prediction-errors \\ --alarm-description \u0026#34;Alert when Lambda has errors\u0026#34; \\ --metric-name Errors \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 300 \\ --threshold 5 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=storm-prediction Troubleshooting Error: \u0026ldquo;Task timed out after 3.00 seconds\u0026rdquo;\nFix: Increase timeout to 60s\nError: \u0026ldquo;Runtime exited with error: signal: killed\u0026rdquo;\nFix: Increase memory to 3008 MB\nError: \u0026ldquo;No module named \u0026rsquo;torch\u0026rsquo;\u0026rdquo;\nFix: Check requirements.txt and rebuild the image\nError: Model cannot be loaded\nFix: Verify the model filename in Dockerfile and app.py match\nEstimated Cost Lambda: Free tier: 1M requests/month, 400,000 GB-seconds After that: $0.20 per 1M requests + $0.0000166667 per GB-second Example: 10,000 requests/month, each request 10s, 3GB RAM Compute: 10,000 × 10s × 3GB × $0.0000166667 = ~$5/month Requests: 10,000 × $0.20/1M = ~$0.002/month Total: ~$5/month API Gateway: $3.50 per million requests 10,000 requests = ~$0.035/month ECR: $0.10 per GB/month storage Image ~2GB = ~$0.20/month Estimated total: ~$5.25/month for 10,000 predictions\nFinal Checklist Fix the model filename mismatch in app.py or Dockerfile Test the Docker image locally Push the image to ECR Create Lambda with 3008MB memory, 60s timeout Create API Gateway and integrate with Lambda Test the API with Postman/curl Update VITE_PREDICTION_API_URL in the frontend Build and deploy the frontend Test the prediction form on the web UI Set up CloudWatch alerts Monitor logs and performance "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Begin foundational learning of Amazon Virtual Private Cloud (VPC) and its core components.\nUnderstand how to design a secure and functional network architecture on AWS.\nPrepare for and actively participate in the GenAI Builder Workshop.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Start with Amazon Virtual Private Cloud (VPC): - Understand the concept and purpose of a VPC as your private network in the AWS cloud. - Learn about CIDR blocks and how to plan your IP address range.\n- Define and differentiate between public subnets and private subnets 29/09/2025 29/09/2025 https://000003.awsstudygroup.com/ 3 Continue with VPC networking components: - Study Route Tables and how they control traffic flow between subnets and to external networks. - Learn about the Internet Gateway (IGW) for public internet access. - Understand the Network Address Translation (NAT) Gateway for outbound internet access from private subnets. 30/09/2025 30/09/2025 https://000003.awsstudygroup.com/ 4 Learn VPC security and connectivity:- Understand Security G roups as a stateful firewall at the instance level. - Learn about Network ACLs as a stateless firewall at the subnet level. - Introduction to VPC Peering for connecting VPCs together. - Begin exploring AWS Site-to-Site VPN concepts for hybrid cloud connections. 01/10/2025 01/10/2025 https://000003.awsstudygroup.com/ 5 - Introduction to VPC Peering for connecting VPCs together. - Begin exploring AWS Site-to-Site VPN concepts for hybrid cloud connections. 02/10/2025 02/10/2025 https://000003.awsstudygroup.com/ 6 Attend and complete GenAI Builder Workshop: - Document key takeaways and newly acquired skills related to AWS AI services. 03/10/2025 03/10/2025 Week 4 Achievements: Built a foundational understanding of Amazon VPC, including its role as an isolated virtual network and the purpose of CIDR blocks for IP planning.\nMastered key VPC components: Learned to create and configure public subnets (with routes to an IGW) and private subnets (with routes to a NAT Gateway), and understood how Route Tables direct traffic.\nGained knowledge of VPC security: Differentiated between Security Groups (for instance-level, stateful protection) and Network ACLs (for subnet-level, stateless rule sets).\nExplored advanced connectivity: Was introduced to VPC Peering for internal AWS connectivity and began learning about AWS Site-to-Site VPN for connecting to on-premises networks.\nSuccessfully prepared for and participated in the GenAI Builder Workshop, gaining hands-on experience with AWS\u0026rsquo;s generative AI tools and understanding their integration into cloud architectures.\nDeveloped initial skills in cloud network design, enabling the planning of a basic, secure multi-tier application architecture using the VPC concepts learned this week.\n"},{"uri":"https://ttjendatgit.github.io/trantiendat/en/5-workshop/","title":"Workshop","tags":[],"description":"","content":"ONLINE PLATFORM FOR TRACKING AND FORECASTING HURRICANE TRAJECTORY Overview Hurricanes are powerful natural disasters that cause severe damage to infrastructure and pose significant risks to human life. Early detection and timely warnings are essential so that people in affected areas have enough time to prepare and evacuate safely.\nTo address this need, our project aims to build an online platform that allows users to freely access information about the most recent storms in the Western Pacific, using data sourced from the trusted NOAA (National Oceanic and Atmospheric Administration). In addition, students, meteorologists, or anyone interested in hurricane dynamics can interact with our system by providing their own input trajectories and receiving predictions generated by our machine learning model.\nThis workshop presents the complete process of building such a model for hurricane forecasting, including several novel time-series techniques—Stepwise Temporal Fading and Plausible Geodesic-Aware Augmentation—as well as a step-by-step explanation of how we built and deployed the platform from scratch.\nWith the support of AWS services such as Amazon S3, AWS Lambda, API Gateway, and CloudFront, we construct a fully serverless architecture. This offers simplicity, scalability, and long-term cost efficiency while ensuring reliable and responsive performance.\nPlatform Architecture The final platform delivers two core functionalities:\nStorm Viewing Users can explore up-to-date information on recent Western Pacific storms, including their historical path, wind speed, temperature, and other relevant parameters.\nHurricane Trajectory Prediction Users can input their own partial storm trajectory and receive a predicted future path generated by our trained model.\nContent Workshop overview Data Preparation ML Model Training Front\u0026amp;Back-End Architect API "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/5-workshop/5.5-platform-api/","title":"Platform API","tags":[],"description":"","content":"BACK-END API DETAIL DESCRIPTION Table of Contents Introduction System Architecture Core Features Technology Stack Project Structure API Endpoints 1. Introduction Weather Backend API is a RESTful service that provides weather information by integrating with OpenWeatherMap API. The backend serves as a middleware layer between frontend applications and external weather data sources.\nFigure 1 2. System Architecture High-Level Architecture ┌─────────────────────────────────────────────────────────────┐ │ Frontend Applications │ │ (React, Mobile, Web Clients) │ └─────────────────────────────────────────────────────────────┘ │ │ HTTPS / REST API ▼ ┌─────────────────────────────────────────────────────────────┐ │ Weather Backend API │ │ (.NET 9.0 - ASP.NET Core) │ ├─────────────────────────────────────────────────────────────┤ │ ┌────────────────┐ ┌────────────────┐ ┌────────────────┐ │ │ Controllers │ │ Services │ │ Program.cs │ │ │ │ │ │ │ - App Startup │ │ │ - WeatherCtrl │ │ - WeatherSvc │ │ - Logging │ │ │ - ForecastCtrl │ │ - Cache Layer │ │ - DI Setup │ │ └────────────────┘ └────────────────┘ └────────────────┘ └─────────────────────────────────────────────────────────────┘ │ │ HTTPS / REST API (External) ▼ ┌─────────────────────────────────────────────────────────────┐ │ External Weather Services │ ├─────────────────────────────────────────────────────────────┤ │ • OpenWeatherMap API │ │ • Redis Caching Layer │ │ • Rate Limiting \u0026amp; Monitoring │ └─────────────────────────────────────────────────────────────┘ 3. Core Features Description: Retrieve current weather conditions for any city worldwide.\nFeatures:\nSearch by city name (e.g., \u0026ldquo;Hanoi\u0026rdquo;, \u0026ldquo;Ho Chi Minh City\u0026rdquo;) Optional country code for precise location Multiple unit systems support (metric, imperial, standard) Multi-language weather descriptions Cached responses for performance API Parameters:\ncityName (required): Name of the city countryCode (optional): ISO 3166 country code units (optional): metric, imperial, or standard language (optional): en, vi, fr, etc. Response Example:\nResponse body Download { \u0026#34;localDate\u0026#34;: \u0026#34;2025-12-06 19:57:02\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Hà Nội\u0026#34;, \u0026#34;coord\u0026#34;: { \u0026#34;lon\u0026#34;: 105.8412, \u0026#34;lat\u0026#34;: 21.0245 }, \u0026#34;weather\u0026#34;: [ { \u0026#34;id\u0026#34;: 804, \u0026#34;main\u0026#34;: \u0026#34;Clouds\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;mây đen u ám\u0026#34;, \u0026#34;icon\u0026#34;: \u0026#34;04n\u0026#34; } ], \u0026#34;main\u0026#34;: { \u0026#34;temp\u0026#34;: 22, \u0026#34;feels_like\u0026#34;: 22.11, \u0026#34;temp_min\u0026#34;: 22, \u0026#34;temp_max\u0026#34;: 22, \u0026#34;pressure\u0026#34;: 1018, \u0026#34;humidity\u0026#34;: 71, \u0026#34;sea_level\u0026#34;: 1018, \u0026#34;grnd_level\u0026#34;: 1017 }, \u0026#34;wind\u0026#34;: { \u0026#34;speed\u0026#34;: 4.14, \u0026#34;deg\u0026#34;: 136, \u0026#34;gust\u0026#34;: 6.84 }, \u0026#34;sys\u0026#34;: { \u0026#34;type\u0026#34;: 1, \u0026#34;id\u0026#34;: 9308, \u0026#34;country\u0026#34;: \u0026#34;VN\u0026#34;, \u0026#34;sunrise\u0026#34;: 1764976827, \u0026#34;sunset\u0026#34;: 1765016103 } } Figure 2 4. Technology Stack Backend Framework .NET 9.0 – Latest .NET runtime ASP.NET Core – Web API framework C# 12 – Primary programming language API Integration HttpClientFactory – Managed HTTP client usage Polly – Retry policies \u0026amp; transient fault handling Newtonsoft.Json / System.Text.Json – JSON serialization Caching \u0026amp; Performance MemoryCache – In-memory caching Redis (optional) – Distributed caching ResponseCompression – Gzip / Brotli compression Development Tools Visual Studio 2022 / VS Code – IDE / Code editor Swagger / OpenAPI – API documentation Git – Version control Docker – Containerization 5. Project Structure WeatherBackend/ │ ├── WeatherBackend.csproj # Project file ├── Program.cs # Application entry point ├── WeatherBackend.http # HTTP request testing file │ ├── appsettings.json # Configuration settings │ ├── Controllers/ # API Controllers │ └── WeatherController.cs # Main weather endpoints │ ├── Services/ # Business logic services │ └── WeatherService/ # Service contracts \u0026amp; implementation 6. API Endpoints Base URL https://localhost:7042/swagger/index.html 6.1 GET /api/weather/current Description: Retrieve the current weather by city name.\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather?city=hanoi\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather?city=hanoi 6.2 GET /api/weather/forecast Description: Get the 5-day weather forecast for a selected city.\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/forecast?city=hochiminh\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather/forecast?city=hochiminh 6.3 GET /api/weather/coordinates Description: Retrieve weather data using latitude and longitude.\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/by-coord?lat=21.0245\u0026amp;lon=105.8412\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather/by-coord?lat=21.0245\u0026amp;lon=105.8412 6.4 GET /api/weather/location Description: Retrieve weather data for the user\u0026rsquo;s current location (requires coordinates from client device).\nCURL Example:\ncurl -X GET \\ \u0026#34;https://localhost:7042/api/Weather/global\u0026#34; \\ -H \u0026#34;accept: */*\u0026#34; Request URL:\nhttps://localhost:7042/api/Weather/global Figure 3 Last Updated: 2025-12-09\nVersion: 1.0.0\nMaintained by: SKYNET\n"},{"uri":"https://ttjendatgit.github.io/trantiendat/en/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Complete studying the remaining AWS network architecture content on the documentation page. Translate and gain a solid understanding of two important technical blog posts. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Absent (Health Reason): - Severely ill with fever, health not fit for work. - Parents took me back to my hometown to rest and recuperate. 06/10/2025 06/10/2025 3 Absent (Health Reason): 07/10/2025 07/10/2025 4 Absent (Health Reason): 07/10/2025 07/10/2025 5 Complete translation tasks: - Returned to the city and resumed work. - Translated the blog post: \u0026ldquo;Gain Compliance Insights into Your AWS Environment with Amazon Q Business\u0026rdquo;, \u0026ldquo;Four ways to grant cross-account access in AWS\u0026rdquo; . 09/10/2025 09/10/2025 6 Complete translation and study content: - Completed studying all remaining content sections about VPC - Compiled and documented knowledge learned about AWS network architecture. 10/10/2025 10/10/2025 https://000003.awsstudygroup.com/vi/ Week 5 Achievements: Successfully completed translation tasks: Accurately and fully translated two important technical blog posts from English to Vietnamese: \u0026ldquo;Gain Compliance Insights into Your AWS Environment with Amazon Q Business\u0026rdquo;: Understood how to integrate AWS Config and Amazon Q Business for natural language compliance analysis. \u0026ldquo;Four ways to grant cross-account access in AWS\u0026rdquo;: Mastered the four methods of granting cross-account access and the security and availability trade-offs of each method. Completed AWS network architecture curriculum: Studied all remaining content on the documentation page, including advanced topics on VPC, hybrid connectivity, and best-practice architectural patterns. "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from September 8, 2025, to December 9, 2025, I gained invaluable experience. I had the opportunity to work with professional platforms and tools, expand my technical knowledge, and, most importantly, received dedicated guidance from mentors and admins, as well as friendly collaboration from peers in the FCJ program.\nThrough team activities and hands-on projects, I was able to apply and enhance my skills, not only in programming, AWS architecture, and diagram design but also in self-directed research, report writing, and teamwork.\nRegarding work ethic, I consistently strived to complete assigned tasks on time, adhered to all regulations, actively communicated with colleagues to optimize work efficiency, and proactively participated in workshops and events organized by the company.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Enhance programming skills to tackle more complex problems.\nExplore and master additional supportive tools and applications to handle tasks more efficiently and optimally.\n"},{"uri":"https://ttjendatgit.github.io/trantiendat/en/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Learn to set up Hybrid DNS using Route 53 Resolver. Study the initial deployment steps for Microsoft Active Directory in a hybrid environment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Study Hybrid DNS concepts \u0026amp; Route 53 Resolver architecture. 13/10/2025 13/10/2025 https://000010.awsstudygroup.com/ 3 Learn to configure Inbound and Outbound Resolver Endpoints. 14/10/2025 14/10/2025 https://000010.awsstudygroup.com/ 4 Configure DNS forwarding rules for hybrid name resolution. 15/10/2025 15/10/2025 https://000010.awsstudygroup.com/ 5 Introduction to Microsoft Active Directory integration. 16/10/2025 16/10/2025 https://000010.awsstudygroup.com/ 6 Study steps for deploying AWS Managed Microsoft AD. 17/10/2025 17/10/2025 https://000010.awsstudygroup.com/ Week 6 Achievements: Gained a solid understanding of Hybrid DNS concepts: Learned how to bridge on-premises and AWS VPC DNS resolution using Route 53 Resolver. Mastered the configuration of Route 53 Resolver endpoints: Understood the purpose, setup, and security configuration for both inbound and outbound endpoints. Successfully learned to implement DNS forwarding rules: Can now direct specific domain queries to designated on-premises DNS servers, enabling seamless name resolution. Acquired foundational knowledge of Microsoft AD integration: Understood the role of Active Directory in hybrid identity management and was introduced to AWS Managed Microsoft AD. Began practical exploration of Managed AD deployment: Studied the initial steps and considerations for deploying a managed Active Directory service within a VPC. "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Focus on reviewing the 4 main pillars of AWS Well-Architected Framework Master key AWS services for the exam Practice labs on security and performance Tasks to be implemented this week: Day Tasks Start Date End Date Reference Materials Mon Secure Architecture Design\n- IAM: Users, Groups, Roles, Policies\n- MFA and authentication methods\n- Service Control Policies (SCPs) for Organizations\n- Encryption: KMS (CMKs, Data Keys), TLS/ACM\n- Review Security Groups vs NACLs\n- AWS Shield \u0026amp; WAF 20/10/2025 20/10/2025 Tue Continue Secure Architectures\n- Amazon GuardDuty: Threat detection\n- AWS Secrets Manager\n- VPC Security: Flow Logs, Endpoints\n- Data encryption at rest \u0026amp; in transit\n- Practice:\n+ Create complex IAM Policies\n+ Configure KMS with CMK\n+ Set up WAF rules 21/10/2025 21/10/2025 Wed Resilient Architecture Design\n- Multi-AZ deployments: RDS, EC2, ELB\n- Multi-Region strategies\n- Disaster Recovery: Backup \u0026amp; Restore, Pilot Light, Warm Standby\n- Auto Scaling: Policies, Lifecycle Hooks\n- Route 53: Routing Policies, Health Checks 22/10/2025 22/10/2025 Thu Continue Resilient Architectures\n- Load Balancing: ALB, NLB, GLB\n- Database resilience: RDS Multi-AZ, Aurora Global Database\n- Storage resilience: S3 Cross-Region Replication\n- Practice:\n+ Configure Auto Scaling group\n+ Set up Multi-AZ RDS\n+ Create Route 53 failover 23/10/2025 23/10/2025 Fri High-Performing Architecture Design\n- Compute scaling: EC2 Auto Scaling, Lambda, Fargate\n- Storage: S3 performance, EFS, EBS types\n- Caching: ElastiCache (Redis/Memcached)\n- Network Optimization: CloudFront, Global Accelerator 24/10/2025 24/10/2025 Week 7 Achievements: Mastered Secure Architectures:\nDeep understanding of IAM: Policies, Roles, Permission Boundaries Proficiency with KMS: CMK, Data Keys, Key Policies Differentiated Security Groups vs NACLs Know how to implement WAF and Shield Proficient in Resilient Architectures:\nDesigned Multi-AZ and Multi-Region architectures Understand Disaster Recovery strategies Configured Auto Scaling and Load Balancing Used Route 53 for failover Successful Practice:\nCreated complex IAM policies Configured KMS encryption Set up Auto Scaling groups Deployed Multi-AZ databases "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nI was truly surprised and impressed by the professionalism here, from the workspace, location, to the office layout—everything was excellent and a completely new experience for me. During the internship, I had the opportunity to interact with the members of FCJ, who were always enthusiastic, ready to support, and answer any questions, even outside of working hours.\n2. Support from Mentor / Team Admin\nWhenever I encountered difficulties, the support from mentors and the team admin was always enthusiastic and thorough. They didn\u0026rsquo;t guide me step-by-step from A to Z, but instead offered suggestions like \u0026ldquo;try doing A this way\u0026rdquo; or \u0026ldquo;consider solving it with approach C.\u0026rdquo; This method helped me gain a deeper understanding of the problems and develop my problem-solving skills.\n3. Relevance of Work to Academic Major\nDevOps was a completely new field for me, and initially, I felt quite overwhelmed, unsure of where to start. However, through learning and hands-on practice, I was able to connect my foundational knowledge with real-world tasks, and I gained valuable experience in a professional working environment.\n4. Learning \u0026amp; Skill Development Opportunities\nLearning and development opportunities at FCJ are always open. The company regularly organizes workshops and professional events, allowing interns to gain a better understanding of the work environment, enhance their knowledge, and apply it to real projects. I learned a great deal, not only professionally but also about maintaining a professional work ethic.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture was a highlight, with clear, detailed regulations that made it easy for everyone to understand and follow. FCJ members were always willing to support each other cheerfully and enthusiastically, never hesitating to share experiences. This made me feel like a genuine part of a professional team.\n6. Internship Policies / Benefits\nThe policies and benefits for interns are excellent, from flexible working hours to organizing professional workshops and events. I not only had the chance to learn from top experts in the field but also received meaningful gifts for participating in activities.\nAdditional Questions What did you find most satisfying during your internship?\nWhat satisfied me most was the opportunity to learn and work in a highly professional environment, along with the dedicated support from FCJ members. Additionally, participating in company-organized events and workshops were invaluable experiences. What do you think the company should improve for future interns?\nI believe the program would be more effective with a structured training roadmap from the very beginning, helping interns better understand the company\u0026rsquo;s actual work and their role in projects. Having step-by-step guidance—from workflow processes to task implementation—would help interns integrate more quickly. If recommending to a friend, would you suggest they intern here? Why or why not?\nI would definitely recommend and encourage my friends to intern here. Experiencing and learning in such a comprehensively professional environment as FCJ will help everyone develop themselves to new heights. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nI hope the process for registering to come to the office can be optimized, for example, through an online registration form where students can select specific days of the week. This would make schedule management clearer, more transparent, and help avoid situations where response is not received in time. Would you like to continue this program in the future?\nIn the future, if given the opportunity, I would very much like to continue participating in the program and will strive to contribute and learn even more. Any other comments (free sharing): "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Complete review of all exam topics Practice mock exams Prepare psychologically and develop test-taking skills Take AWS Certification exam Tasks to be implemented this week: Day Tasks Start Date End Date Reference Materials Mon Continue High-Performing Architectures\n- Database performance: RDS Read Replicas, Aurora\n- Lambda optimization: Memory, Concurrency, Cold Start\n- Practice:\n+ Configure CloudFront distribution\n+ Create ElastiCache cluster\n+ Optimize Lambda functions 27/10/2025 27/10/2025 Tue Cost-Optimized Architecture Design\n- Cost Explorer and Budgets\n- Savings Plans vs Reserved Instances\n- Lifecycle Policies: S3, EBS\n- NAT Gateway optimization\n- Storage Tiering: S3, Glacier\n- Right-sizing instances 28/10/2025 28/10/2025 Wed Review AWS Well-Architected Framework\n- 6 Pillars: Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, Sustainability\n- Review key services:\n+ EC2: Instance types, AMIs, Placement Groups\n+ S3: Classes, Features, Versioning\n+ RDS: Engines, Features, Backups 29/10/2025 29/10/2025 Thu Mock Exams and Final Review\n- Complete 2 full-length practice exams\n- Analyze incorrect answers\n- Review weak areas:\n+ Advanced VPC networking\n+ Database migration strategies\n+ Monitoring with CloudWatch\n- Prepare exam day materials 30/10/2025 30/10/2025 Fri EXAM DAY - 31/10/2025\n- Exam\n- Quick final review before exam\n- Check equipment and internet connection\n- Time management during exam\n- Complete the exam\n- Await results 31/10/2025 31/10/2025 Week 8 Achievements: Completed Exam Knowledge: Mastered 6 Pillars of Well-Architected Framework Deep understanding of Cost Optimization strategies Proficient in Performance Optimization techniques Reviewed all key services "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Research and identify suitable AWS services for building a weather API. Design and finalize the complete API structure using Swagger/OpenAPI specification. Collaborate with the Frontend team to align on API design. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 AWS Services Research: - Research AWS API Gateway for API management and creation. - Identify AWS Lambda as the serverless compute layer. - Review external weather data providers and API integration methods. 03/11/2025 03/11/2025 2 Define API Endpoints \u0026amp; Data Flow: - Define the 4 core API endpoints - Design request/response schemas for each endpoint. 04/11/2025 04/11/2025 3 Data Models \u0026amp; External Integration: - Define the data model for the weather response (temp, humidity, condition, etc.). - Define the data model for the 5-day forecast response. - Research and select a free weather API (e.g., OpenWeatherMap). - Plan the integration logic within the Lambda function. 05/11/2025 05/11/2025 4 Swagger/OpenAPI Specification Draft: - Start writing the Swagger (OpenAPI 3.0) YAML file. - Define the paths object with all 4 endpoints. - Specify parameters (path, query) and request examples. - Define the response schemas and examples for success/error cases. 06/11/2025 06/11/2025 5 Collaboration \u0026amp; Version Control: - Collaborate with the Frontend team to review and finalize the API endpoints and data models. - Make necessary adjustments to the Swagger spec based on feedback. 07/11/2025 07/11/2025 Week 9 Achievements: Researched and selected AWS API Gateway and Lambda as the foundation for the weather API. Designed and documented four core API endpoints for weather data retrieval. Successfully collaborated with the Frontend team to align the API design. "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Focus on enhancing and optimizing the performance of the weather API. Attend the AWS Cloud Mastery Series #1 event. Implement improvements to make the API more robust and efficient. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Implement API Caching: - Research and implement caching strategies using Amazon ElastiCache (Redis) or API Gateway caching. - Apply caching for frequently requested weather data (e.g., by city) to reduce external API calls and latency. 10/11/2025 10/11/2025 3 Error Handling \u0026amp; Retry Logic: - Improve error handling in Lambda functions for external API failures. - Implement retry logic with exponential backoff for transient errors. - Define clear error response formats for the API. 11/11/2025 11/11/2025 4 Optimize Lambda Performance: - Review and adjust Lambda function configurations (memory, timeout). - Implement Lambda layers for shared dependencies (e.g., weather API client). - Optimize code for cold start reduction. 12/11/2025 12/11/2025 5 API Monitoring \u0026amp; Logging: - Set up Amazon CloudWatch Logs for Lambda function logging. - Create CloudWatch Alarms for error rates and high latency. - Implement structured logging for easier debugging. 13/11/2025 13/11/2025 CloudWatch 6 Security \u0026amp; Rate Limiting: - Configure API Gateway usage plans and API keys for basic rate limiting. - Review and tighten IAM roles and policies for Lambda functions. - Ensure secure handling of external API keys using AWS Secrets Manager. 14/11/2025 14/11/2025 API Gateway, IAM, Secrets Manager 7 Attend AWS Cloud Mastery Series #1: - Participate in the event to learn about advanced AWS services and architectures. - Document key takeaways relevant to the project. 15/11/2025 15/11/2025 Week 10 Achievements: Enhanced API robustness with improved error handling and retry mechanisms. Optimized Lambda functions for better performance and manageability. Established monitoring and logging for operational visibility. Improved API security with rate limiting and secure credential management. Gained new insights from the AWS Cloud Mastery Series event. "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Participate in AWS community events and workshops to enhance knowledge. Complete practical labs to strengthen AWS skills through hands-on experience. Focus on serverless and AI/ML topics relevant to project development. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Attend AWS Cloud Mastery Series #2: - Participate in the second session of the AWS Cloud Mastery series. - Learn about advanced cloud concepts and best practices. - Network with cloud professionals and share experiences. 17/11/2025 17/11/2025 3 - Understand state machines, error handling, and integration patterns. - Apply concepts to potential workflow automation in the project. 18/11/2025 18/11/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Continue with labs: Amazon EventBridge and serverless event-driven architecture. - Learn to build decoupled systems using events. - Explore how EventBridge can be used for project notifications. 19/11/2025 19/11/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 5 Attend Amazon Q Workshop: - Participate in the hands-on Amazon Q workshop. - Learn practical applications of generative AI in business contexts. - Explore how Q can be integrated into development workflows. 20/11/2025 20/11/2025 6 Participate in AWS GenAI Game Day: - Join the AWS GenAI Game Day competition. - Apply AI/ML knowledge in a practical, gamified environment. - Collaborate with team members to solve challenges. 21/11/2025 21/11/2025 Week 11 Achievements: Successfully attended AWS Cloud Mastery #2, gaining deeper insights into advanced AWS architectures. Completed practical labs on AWS Step Functions and EventBridge, strengthening serverless workflow skills. Gained hands-on experience with Amazon Q, understanding its potential applications in development. Participated in AWS GenAI Game Day, applying AI/ML knowledge in competitive team challenges. Developed practical skills that can be directly applied to current and future cloud projects. "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Collaborate with the team to complete the final stages of the project. Document all events participated in during the internship. Review and finalize the worklog documentation. Attend AWS Cloud Mastery Series #3. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Final Project Work with Team: - Conduct end-to-end testing of all features. - Fix bugs and optimize performance based on testing feedback. 24/11/2025 24/11/2025 3 Final Project Work with Team: - Prepare final project documentation and README file. 25/11/2025 25/11/2025 4 Final Project Work with Team: - Final code review and merge to main branch. - Prepare final presentation materials. 26/11/2025 26/11/2025 5 Event Documentation: - Write detailed summaries of all attended events and workshops. - Document key learnings and takeaways from each event. - Organize event certificates and materials. 27/11/2025 27/11/2025 6 Worklog Review \u0026amp; Finalization: - Review and edit all weekly worklog entries. - Ensure consistency and accuracy across all documentation. - Add any missing achievements or reflections. 28/11/2025 28/11/2025 7 Attend AWS Cloud Mastery Series #3: - Participate in the final session of the AWS Cloud Mastery series. - Learn about advanced AWS architectures and best practices. - Network with cloud professionals and share project experiences. 29/11/2025 29/11/2025 Week 12 Achievements: Completed final project development, testing, and documentation with the team. Documented learnings from all attended events. Finalized the complete 12-week worklog. Attended the final AWS Cloud Mastery Series session "},{"uri":"https://ttjendatgit.github.io/trantiendat/en/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://ttjendatgit.github.io/trantiendat/en/tags/","title":"Tags","tags":[],"description":"","content":""}]